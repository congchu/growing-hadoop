
#### 가상 머신 중단 및 VDI 이미지 백업하기


로그아웃 -> 다시 root 계정
~~~
[1]$ exit
~~~
디스크 용량 확인
~~~
[2]$ df
~~~
전원끄기 신호 보내기
~~~
[3]$ shutdown  -h  now
~~~

#### VDI 파일 백업하기

~~~
[hadoop@client ~]$ exit
logout
[root@client ~]# df
Filesystem     1K-blocks    Used Available Use% Mounted on
/dev/sda3        3770368 2867824    902544  77% /
devtmpfs          586868       0    586868   0% /dev
tmpfs             596464       0    596464   0% /dev/shm
tmpfs             596464    7784    588680   2% /run
tmpfs             596464       0    596464   0% /sys/fs/cgroup
/dev/sda1         201380   94904    106476  48% /boot
tmpfs             119296       0    119296   0% /run/user/0

[root@client ~]# shutdown  -h  now
~~~



#### 리눅스 환경 설정
=> 이제부터는 root가 아닌 hadoop 계정으로 작업합니다.

~~~
[1]$ cp  .bashrc  org_bashrc
[2]$ cp  bashrc2.txt  .bashrc
[3]$ source  .bashrc
[4]$ cat  .bashrc
[5]$ env
~~~

* /home/hadoop/.bashrc : hadoop 계정의 환경 설정 파일
* env : 환경 설정 조회 프로그램

~~~
[hadoop@client ~]$ cp  .bashrc  org_bashrc
[hadoop@client ~]$ cp  bashrc2.txt  .bashrc
[hadoop@client ~]$ source  .bashrc
[hadoop@client ~]$ cat .bashrc

# .bashrc

if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
~~~
~~~
# HADOOP Config Start
pathmunge () {
    case ":${PATH}:" in
        *:"$1":*)
            ;;
        *)
            if [ "$2" = "after" ] ; then
                PATH=$PATH:$1
            else
                PATH=$1:$PATH
            fi
    esac
}

export JAVA_HOME=/usr/local/java
export CLASSPATH=/usr/local/java/jre/lib/*
pathmunge /usr/local/java before
pathmunge /usr/local/java/bin before

export BASEHOME=/home/hadoop

export HADOOP_PREFIX=$BASEHOME/hadoop
export HADOOP_HOME=$BASEHOME/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
export HADOOP_CLASSPATH=$JAVA_HOME/lib/tools.jar

export PIG_HOME=$BASEHOME/pig
export PIG_CLASSPATH=$BASEHOME/hadoop/conf

export HIVE_HOME=$BASEHOME/hive
export HIVE_CONF_DIR=$BASEHOME/hive/conf
export HIVE_CLASS_PATH=$HIVE_CONF_DIR
export HADOOP_USER_CLASSPATH_FIRST=true

export SPARK_HOME=/home/hadoop/spark
export SPARK_CONF_DIR=$SPARK_HOME/conf

#pathmunge $BASEHOME/sqoop/bin
pathmunge $BASEHOME/pig/bin
pathmunge $BASEHOME/hive/bin
pathmunge $BASEHOME/hadoop/bin
~~~
HADOOP Config End

~~~
[hadoop@client ~]$ env
SPARK_HOME=/home/hadoop/spark
HOSTNAME=localhost.localdomain
PIG_HOME=/home/hadoop/pig
SHELL=/bin/bash
TERM=xterm
HIVE_CLASS_PATH=/home/hadoop/hive/conf
HADOOP_HOME=/home/hadoop/hadoop
HISTSIZE=1000
HADOOP_PREFIX=/home/hadoop/hadoop
SPARK_CONF_DIR=/home/hadoop/spark/conf
YARN_HOME=/home/hadoop/hadoop
USER=hadoop
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.tbz=01;31:*.tbz2=01;31:*.bz=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:
HADOOP_COMMON_LIB_NATIVE_DIR=/home/hadoop/hadoop/lib/native
MAIL=/var/spool/mail/hadoop
PATH=/home/hadoop/hadoop/bin:/home/hadoop/hive/bin:/home/hadoop/pig/bin:/usr/local/java/bin:/usr/local/java:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin
HADOOP_HDFS_HOME=/home/hadoop/hadoop
HIVE_HOME=/home/hadoop/hive
HADOOP_COMMON_HOME=/home/hadoop/hadoop
PWD=/home/hadoop
JAVA_HOME=/usr/local/java
HADOOP_CLASSPATH=/usr/local/java/lib/tools.jar
HADOOP_INSTALL=/home/hadoop/hadoop
PIG_CLASSPATH=/home/hadoop/hadoop/conf
HADOOP_CONF_DIR=/home/hadoop/hadoop/etc/hadoop
LANG=ko_KR.UTF-8
HADOOP_OPTS=-Djava.library.path=/home/hadoop/hadoop/lib/native
HISTCONTROL=ignoredups
SHLVL=1
HOME=/home/hadoop
HADOOP_MAPRED_HOME=/home/hadoop/hadoop
LOGNAME=hadoop
CLASSPATH=/usr/local/java/jre/lib/*
LESSOPEN=||/usr/bin/lesspipe.sh %s
BASEHOME=/home/hadoop
HADOOP_USER_CLASSPATH_FIRST=true
G_BROKEN_FILENAMES=1
HIVE_CONF_DIR=/home/hadoop/hive/conf
_=/bin/env
~~~

JDK 버전 확인
~~~
[6]$ javac -version
~~~

현재 실행되는 자바 프로세스 조회(jps만 나옴)
~~~
[7]$ jps
~~~
=> 현재 머신에서 실행되고 있는 자바 프로그램 조회하기
~~~
[hadoop@client ~]$ javac -version
javac 1.7.0_55

[hadoop@client ~]$ jps
1657 Jps
~~~

[Note] 1657 번호는 리눅스에서 실행되는 프로세스의
       Random ID 값(각자 다른 값을 가짐)


#### SSH 설정 : .ssh 디렉터리 생성하기

-> 클러스터의 수십,수백대의 머신에 접속하기 위해서

-> 가상머신을 복제하여 총 3대를 만든 후
   비밀번호 없이 SSH 접속이 가능하도록 설정

[NOTE] 처음 ssh 명령을 실행할 때 비밀번호를 물어보면 그냥 Enter를 3번.
~~~
[8]$ ssh localhost

[9]$ ls -al .ssh

[10]$ rm .ssh/known_hosts  
~~~

~~~
[hadoop@client ~]$ ssh localhost
Warning: Permanently added 'localhost' (RSA) to the list of known hosts.
hadoop@localhost's password:
Permission denied, please try again.
hadoop@localhost's password:
Permission denied, please try again.
hadoop@localhost's password:
Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).
~~~
~~~
[hadoop@client ~]$ ls -al  .ssh
합계 12
drwx------. 2 hadoop hadoop 4096 2017-05-06 16:50 .
drwx------. 3 hadoop hadoop 4096 2017-05-06 16:50 ..
-rw-r--r--. 1 hadoop hadoop  391 2017-05-06 16:50 known_hosts
[hadoop@client ~]$ rm .ssh/known_hosts
~~~
#### (4) 아파치 하둡 설정
###### 하둡 설정
하둡 설정 디렉터리 : /home/hadoop/hadoop/etc/hadoop

=> 다음에 나오는 7개의 파일을 편집하세요.
~~~
[2]$   hadoop-env.sh
       core-site.xml
       hdfs-site.xml
       mapred-site.xml
       yarn-site.xml
       log4j.properties
       slaves
~~~
1. 하둡 환경 설정 파일 : hadoop-env.sh
~~~
export JAVA_HOME=/usr/local/java

export HADOOP_PREFIX=${HADOOP_HOME}

export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native

export HADOOP_OPTS="${HADOOP_OPTS} -XX:-PrintWarnings -Djava.library.path=${HADOOP_PREFIX}/lib/native"
~~~

2. 하둡 코어 설정 : core-site.xml

- 네임노드 -> hdfs://namenode:8020/
~~~
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
 <property>
  <name>fs.defaultFS</name>
   <value>hdfs://namenode.hadoop.kr:8020/</value>
 </property>
 <property>
  <name>io.file.buffer.size</name>
   <value>131072</value>
 </property>
</configuration>
~~~

3. HDFS 설정 : hdfs-site.xml

- 복제인수는 1->3으로 변경해야 함
- HDFS 네임노드 웹UI 호스트명/포트           ->    namenode:50070
- HDFS 보조네임노드 웹UI 호스트명/포트       ->  secondnode:50090
- HDFS 데이터노드 웹UI 호스트명/포트         ->    datanode:50010 * 잘 안됨
~~~
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
 <property>
  <name>dfs.replication</name>
  <value>1</value>
 </property>
 <property>
  <name>dfs.namenode.http-address</name>
  <value>namenode.hadoop.kr:50070</value>
 </property>
 <property>
  <name>dfs.namenode.secondary.http-address</name>
  <value>secondnode.hadoop.kr:50090</value>
 </property>
 <property>
  <name>dfs.datanode.http-address</name>
  <value>0.0.0.0:50010</value>
 </property>
 <property>
  <name>dfs.namenode.name.dir</name>
   <value>file:///data/name1,file:///data/name2</value>
 </property>
 <property>
  <name>dfs.datanode.data.dir</name>
   <value>file:///data/data</value>
 </property>
 <property>
  <name>dfs.namenode.checkpoint.dir</name>
   <value>file:///data/namesecondary</value>
 </property>
</configuration>
~~~

4. MapReduce 2.0 설정 : mapred-site.xml
~~~
- mapreduce.jobhistory.address(IPC Port)  -> namenode:10020
- MapReduce 히스토리 서버 호스트명/포트   -> namenode:19888

<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
 <property>
  <name>mapreduce.framework.name</name>
   <value>yarn</value>
 </property>
 <property>
  <name>mapreduce.jobhistory.address</name>
   <value>namenode.hadoop.kr:10020</value>
 </property>
 <property>
  <name>mapreduce.jobhistory.webapp.address</name>
   <value>namenode.hadoop.kr:19888</value>
 </property>
</configuration>
~~~
5. YARN 설정 : yarn-site.xml

- YARN 리소스매니저 웹UI 호스트명/포트 -> secondnode:8088
- YARN 노드매니저 웹UI 호스트명/포트   -> namenode:8042 secondnode:8042 datanode(1-N):8042
~~~
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
 <property>
  <name>yarn.resourcemanager.hostname</name>
    <value>secondnode.hadoop.kr</value>
 </property>
 <property>
  <name>yarn.nodemanager.local-dirs</name>
    <value>/data/nm</value>
  </property>
 <property>
  <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
 </property>
 <property>
  <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>2</value>
 </property>
</configuration>
~~~

6. 하둡 로그 설정 : log4j.properties

- 로그 수준을 INFO에서 ERROR으로 변경해야 함

~~~
hadoop.root.logger=ERROR,console
~~~

7. 워커노드 설정 : slaves
~~~
datanode1.hadoop.kr
datanode2.hadoop.kr
~~~

**설정이 잘 안되면 다음 방법을 사용하세요!**

- 클러스터모드 하둡 설정 파일 다운로드
~~~
[0]$ cd
[1]$ wget http://www.db21.co.kr/hadoop/hdconf_2017.tgz
~~~
- 압축 풀기
~~~
[2]$ tar xvzf hdconf_2017.tgz
~~~
- 내려받은 설정 파일 확인
~~~
[3]$ ll etc_hadoop_2017
~~~
- 설정 파일을 하둡 설정 디렉터리에 덮어쓰기
~~~
[4]$ cp etc_hadoop_2017/*  hadoop/etc/hadoop/

[5]$ echo 'datanode1.hadoop.kr' > hadoop/etc/hadoop/slaves
     echo 'datanode2.hadoop.kr' >> hadoop/etc/hadoop/slaves

[6]$ cat hadoop/etc/hadoop/slaves
~~~

~~~
[hadoop@client ~]$ wget http://www.db21.co.kr/hadoop/hdconf_2017.tgz
--2017-06-01 14:27:00--  http://www.db21.co.kr/hadoop/hdconf_2017.tgz
Resolving www.db21.co.kr (www.db21.co.kr)... 121.124.124.244
Connecting to www.db21.co.kr (www.db21.co.kr)|121.124.124.244|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4465 (4.4K) [application/x-tar]
Saving to: ‘hdconf_2017.tgz’

100%[======================================>] 4,465       --.-K/s   in 0.006s

2017-06-01 14:27:00 (788 KB/s) - ‘hdconf_2017.tgz’ saved [4465/4465]

[hadoop@client ~]$  tar xvzf hdconf_2017.tgz
etc_hadoop_2017/
etc_hadoop_2017/hadoop-env.sh
etc_hadoop_2017/yarn-site.xml
etc_hadoop_2017/hdfs-site.xml
etc_hadoop_2017/core-site.xml
etc_hadoop_2017/log4j.properties
etc_hadoop_2017/mapred-site.xml

[hadoop@client ~]$ ll etc_hadoop_2017
합계 32
-rw-r--r--. 1 hadoop hadoop   321  5월 25 16:56 core-site.xml
-rw-r--r--. 1 hadoop hadoop  2737  5월 25 16:57 hadoop-env.sh
-rw-r--r--. 1 hadoop hadoop   862  5월 25 16:56 hdfs-site.xml
-rw-r--r--. 1 hadoop hadoop 11292  5월 25 16:57 log4j.properties
-rw-rw-r--. 1 hadoop hadoop   453  6월  1 14:25 mapred-site.xml
-rw-r--r--. 1 hadoop hadoop   545  5월 25 16:56 yarn-site.xml

[hadoop@client ~]$ cp etc_hadoop_2017/*  hadoop/etc/hadoop/

[hadoop@client ~]$ echo 'datanode1.hadoop.kr' > hadoop/etc/hadoop/slaves
[hadoop@client ~]$ echo 'datanode2.hadoop.kr' >> hadoop/etc/hadoop/slaves

[hadoop@client ~]$ cat hadoop/etc/hadoop/slaves                    
datanode1.hadoop.kr
datanode2.hadoop.kr
~~~

#### (5) 가상 머신 복제하기

로그아웃 => hadoop 계정에서 다시 root 계정으로
~~~
[1]$ exit
~~~
root 계정 확인
~~~
[2]$ whoami
~~~
가상머신 종료
~~~
[3]$ shutdown  -h  now
~~~

=> Virtual Box에서 작업합니다.

클러스터 1번 hd1-client 머신을 선택한 후

[복제]를 통해서 가상 머신 2대 만들기

**클러스터 2번**

새 머신 이름 : hd2-namenode
[v] 모든 네트워크 카드의 MAC 주소 초기화

복제 방식 : 완전한 복제

**클러스터 3번**

새 머신 이름 : hd3-secondnode
[v] 모든 네트워크 카드의 MAC 주소 초기화

복제 방식 : 완전한 복제

[NOTE] 복제된 2번,3번 머신의 네트워크 설정은 추가로 할 필요가 없음
       - MAC 주소 신경쓰지 마세요

#### (6) 가상 머신 고정 IP 설정(콘솔에서)

=> 가상 머신 3대를 모두 시작( 전원 On )

**클러스터 1번 머신 [ hd1-client ]**

=> 가상 머신 콘솔에서 직접
~~~
[1]$ ip  a
[2]$ ifconfig  enp0s8  192.168.56.21
[3]$ ip  a
~~~

**클러스터 2번 머신 [ hd2-namenode ]**

=> 가상 머신 콘솔에서 직접
~~~
[1]$ ip  a
[2]$ ifconfig  enp0s8  192.168.56.31
[3]$ ip  a
~~~
** 클러스터 3번 머신 [ hd3-secondnode ]**

=> 가상 머신 콘솔에서 직접
~~~
[1]$ ip  a
[2]$ ifconfig  enp0s8  192.168.56.41
[3]$ ip  a
~~~
#### (7) 클러스터 3대 머신에 개별 네트워크 설정

=> putty.exe로 클러스터 1번 머신 [ hd1-client ]에 ssh 접속

- IP : 192.168.56.21

=> 자신의 호스트 이름 설정 및 부팅시 고정 IP 사용하기 설정

**클러스터 1번 머신 [ hd1-client ]**

~~~
[1]$ hostname
[2]$ echo 'ifconfig  enp0s8  192.168.56.21' >> /etc/rc.d/rc.local
[3]$ chmod  +x  /etc/rc.d/rc.local
~~~

[NOTE] /etc/rc.d/rc.local 파일에 부팅시 자동으로 실행할 명령어를 추가



**클러스터 2번 머신 [ hd2-namenode ]**

=> client에서 2번 머신으로 접속
~~~
[4]$ ssh namenode
~~~
호스트명을 변경하고 확인
~~~
[5]$ echo 'namenode.hadoop.kr' > /etc/hostname
[6]$ hostname namenode.hadoop.kr
[7]$ hostname
~~~
부팅시 고정 IP 사용하기 설정
~~~
[8]$ echo 'ifconfig  enp0s8  192.168.56.31' >> /etc/rc.d/rc.local
[9]$ chmod  +x  /etc/rc.d/rc.local
[10]$ w
[11]$ exit
~~~
=> 다시 들어가서 Prompt가 [root@namenode ~]# 으로 변경된 것을 확인
~~~
[12]$ ssh namenode
[13]$ exit
~~~
=> 2번 머신에서 로그아웃하여 client로 돌아감

~~~
[root@client ~]# ssh namenode
Warning: Permanently added 'namenode,192.168.56.31' (ECDSA) to the list of known hosts.
root@namenode's password:
Last login: Thu Jun  1 16:46:31 2017

[root@client ~]# echo 'namenode.hadoop.kr' > /etc/hostname
[root@client ~]# hostname namenode.hadoop.kr
[root@client ~]# hostname
namenode.hadoop.kr

[root@client ~]# echo 'ifconfig  enp0s8  192.168.56.31' >> /etc/rc.d/rc.local
[root@client ~]# chmod  +x  /etc/rc.d/rc.local

[root@client ~]# w
 17:00:25 up 15 min,  2 users,  load average: 0.00, 0.01, 0.04
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:46   13:37   0.04s  0.04s -bash
root     pts/0    client.hadoop.kr 16:57    1.00s  0.02s  0.01s w
[root@client ~]# exit
logout
Connection to namenode closed.
[root@client ~]#


[root@client ~]# ssh namenode
root@namenode's password:
Last login: Thu Jun  1 16:57:28 2017 from client.hadoop.kr
[root@namenode ~]# exit
logout
Connection to namenode closed.
~~~

? 여기는 어디인가 ?
~~~
[root@client ~]# w
 17:08:16 up 13 min,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:55   12:56   0.02s  0.02s -bash
root     pts/0    192.168.56.1     16:55    8.00s  0.04s  0.03s w
~~~

**클러스터 3번 머신 [ hd3-secondnode ]**

=> client에서 3번 머신으로 접속

~~~
[14]$ ssh secondnode
~~~

호스트명을 변경하고 확인
~~~
[15]$ echo 'secondnode.hadoop.kr' > /etc/hostname
[16]$ hostname secondnode.hadoop.kr
[17]$ hostname
~~~
부팅시 고정 IP 사용하기 설정
~~~
[18]$ echo 'ifconfig  enp0s8  192.168.56.41' >> /etc/rc.d/rc.local
[19]$ chmod  +x  /etc/rc.d/rc.local
[20]$ w
[21]$ exit
~~~
=> 다시 들어가서 Prompt가 [root@secondnode ~]# 으로 변경된 것을 확인
~~~
[22]$ ssh secondnode
[23]$ exit
~~~
=> 3번 머신에서 로그아웃하여 client로 돌아감


~~~
[root@client ~]# ssh secondnode
Warning: Permanently added 'secondnode,192.168.56.41' (ECDSA) to the list of known hosts.
root@secondnode's password:
Last login: Thu Jun  1 16:47:23 2017

[root@client ~]# echo 'secondnode.hadoop.kr' > /etc/hostname
[root@client ~]# hostname secondnode.hadoop.kr
[root@client ~]# hostname
secondnode.hadoop.kr

[root@client ~]# echo 'ifconfig  enp0s8  192.168.56.41' >> /etc/rc.d/rc.local
[root@client ~]# chmod  +x  /etc/rc.d/rc.local

[root@client ~]# w
 17:10:32 up 23 min,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:47   22:56   0.04s  0.04s -bash
root     pts/0    client.hadoop.kr 17:08    0.00s  0.02s  0.01s w
[root@client ~]# exit
logout
Connection to secondnode closed.


[root@client ~]# ssh secondnode
root@secondnode's password:
Last login: Thu Jun  1 17:08:50 2017 from client.hadoop.kr

[root@secondnode ~]# exit
logout
Connection to secondnode closed.
[root@client ~]#
~~~
####(8) 비밀번호 없이 클러스터의 다른 머신에 접속하기 설정


개인키와 공개키를 만들고 접속대상 머신의 키박스에 공개키를 등록

=> 개인키(열쇠) - 공개키(자물쇠) : id_dsa - id_dsa.pub

=> 키박스에 다수의 공개키를 등록할 수 있음 : authorized_keys


=---> 이제부터는 hadoop 계정으로 하는 작업입니다.
~~~
[1]$ su  -  hadoop
[2]$ whoami
[3]$ hostname
~~~

~~~
[root@client ~]# su - hadoop
마지막 로그인: 목  6월  1 16:18:07 KST 2017 일시 pts/0

[hadoop@client ~]$ whoami
hadoop
[hadoop@client ~]$ hostname
client.hadoop.kr
[hadoop@client ~]$
~~~

**클러스터 1번 머신 [ hd1-client ]**

개인키와 공개키를 생성
~~~
[1]$ ssh-keygen   -t    dsa    -P    ''    -f    ~/.ssh/id_dsa
~~~

공개키를 키박스에 추가
~~~
[2]$ cat   ~/.ssh/id_dsa.pub   >>  ~/.ssh/authorized_keys
~~~

접근권한 변경 : 다른 사람이 보면 안되므로
~~~
[3]$ chmod   400   ~/.ssh/authorized_keys
~~~
=> 읽기만 할 수 있도록 접근 권한을 변경해야 함

- 확인하기
~~~
[4]$ ls -al  .ssh
[5]$ ssh localhost
     w
     exit
~~~

~~~
[hadoop@client ~]$ ssh-keygen   -t    dsa    -P    ''    -f    ~/.ssh/id_dsa
Generating public/private dsa key pair.
Your identification has been saved in /home/hadoop/.ssh/id_dsa.
Your public key has been saved in /home/hadoop/.ssh/id_dsa.pub.
The key fingerprint is:
dd:ec:0f:38:06:ef:ba:46:8f:1c:2e:5e:a6:70:d5:fd hadoop@client.hadoop.kr
The key's randomart image is:
+--[ DSA 1024]----+
|                 |
|                 |
|                 |
|         o +     |
|        S o +    |
|       .oo o .   |
|    . .+o+= o E  |
|     o.+=o.. o   |
|     .oooo.   .  |
+-----------------+
[hadoop@client ~]$ cat   ~/.ssh/id_dsa.pub   >>  ~/.ssh/authorized_keys
[hadoop@client ~]$ chmod   400   ~/.ssh/authorized_keys
[hadoop@client ~]$ ls -al  .ssh
합계 16
drwx------. 2 hadoop hadoop   61  6월  1 17:30 .
drwx------. 8 hadoop hadoop 4096  6월  1 16:27 ..
-r--------. 1 hadoop hadoop  613  6월  1 17:30 authorized_keys
-rw-------. 1 hadoop hadoop  672  6월  1 17:30 id_dsa
-rw-r--r--. 1 hadoop hadoop  613  6월  1 17:30 id_dsa.pub
[hadoop@client ~]$ ssh localhost
Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
Last login: Thu Jun  1 17:29:40 2017
[hadoop@client ~]$ w
 17:31:28 up 36 min,  3 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:55   36:08   0.02s  0.02s -bash
root     pts/0    192.168.56.1     16:55    0.00s  0.05s  0.01s ssh localhos
hadoop   pts/1    localhost        17:31    0.00s  0.01s  0.01s w
[hadoop@client ~]$ exit
logout
Connection to localhost closed.
~~~

1번 머신(client)의 공개키를 2번,3번 머신에 전송

~~~
[6]$ scp ~/.ssh/id_dsa.pub  hadoop@namenode://home/hadoop/.ssh/client.pub
[7]$ scp ~/.ssh/id_dsa.pub  hadoop@secondnode://home/hadoop/.ssh/client.pub
~~~

**클러스터 2번 머신 [ hd2-namenode ]**
~~~
[1]$ ssh namenode
~~~
개인키와 공개키를 생성
~~~
[2]$ ssh-keygen   -t    dsa    -P    ''    -f    ~/.ssh/id_dsa
~~~
공개키 이름을 식별하기 쉽게 변경
~~~
[3]$ mv ~/.ssh/id_dsa.pub ~/.ssh/namenode.pub
~~~

공개키를 3번 머신에만 전송
~~~
[4-1]$ scp ~/.ssh/namenode.pub  hadoop@secondnode://home/hadoop/.ssh/namenode.pub
~~~
- 공개키를 다른 워커에도 전송
~~~
[4-2]$ scp ~/.ssh/namenode.pub  hadoop@datanode1://home/hadoop/.ssh/namenode.pub
[4-3]$ scp ~/.ssh/namenode.pub  hadoop@datanode2://home/hadoop/.ssh/namenode.pub
[5]$ exit
~~~

~~~
[hadoop@client ~]$ ssh namenode
hadoop@namenode's password:
Last login: Thu Jun  1 17:35:20 2017

[hadoop@namenode ~]$ ssh-keygen   -t    dsa    -P    ''    -f    ~/.ssh/id_dsa
Generating public/private dsa key pair.
Your identification has been saved in /home/hadoop/.ssh/id_dsa.
Your public key has been saved in /home/hadoop/.ssh/id_dsa.pub.
The key fingerprint is:
ec:f7:fe:b4:e1:ab:a4:3d:a8:72:50:db:8c:60:4a:14 hadoop@namenode.hadoop.kr
The key's randomart image is:
+--[ DSA 1024]----+
|    E.           |
|    .            |
|   .             |
|    . o..        |
|   . o oS=       |
|    . ..o o      |
|       .. ... o  |
|      . ...=.o o |
|       o....=+=. |
+-----------------+

[hadoop@namenode ~]$ mv ~/.ssh/id_dsa.pub ~/.ssh/namenode.pub

[hadoop@namenode ~]$ scp ~/.ssh/namenode.pub  hadoop@secondnode://home/hadoop/.ssh/namenode.pub
Warning: Permanently added 'secondnode,192.168.56.41' (ECDSA) to the list of known hosts.
hadoop@secondnode's password:
namenode.pub                               100%  615     0.6KB/s   00:00

[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$
~~~

**클러스터 3번 머신 [ hd3-secondnode ]**
~~~
[1]$ ssh secondnode
~~~

개인키와 공개키를 생성
~~~
[2]$ ssh-keygen   -t    dsa    -P    ''    -f    ~/.ssh/id_dsa
~~~

공개키 이름을 식별하기 쉽게 변경
~~~
[3]$ mv ~/.ssh/id_dsa.pub ~/.ssh/secondnode.pub
~~~

공개키를 2번 머신에만 전송
~~~
[4]$ scp ~/.ssh/secondnode.pub  hadoop@namenode://home/hadoop/.ssh/secondnode.pub
[5]$ exit
~~~
~~~
[hadoop@client ~]$ ssh secondnode
Warning: Permanently added 'secondnode,192.168.56.41' (ECDSA) to the list of known hosts.
hadoop@secondnode's password:
Last failed login: Thu Jun  1 16:21:15 KST 2017 from localhost on ssh:notty
There were 2 failed login attempts since the last successful login.
Last login: Thu Jun  1 16:18:07 2017

[hadoop@secondnode ~]$ ssh-keygen   -t    dsa    -P    ''    -f    ~/.ssh/id_dsa
Generating public/private dsa key pair.
Your identification has been saved in /home/hadoop/.ssh/id_dsa.
Your public key has been saved in /home/hadoop/.ssh/id_dsa.pub.
The key fingerprint is:
46:af:4c:6f:94:ab:17:bb:4b:67:83:42:83:56:f1:3e hadoop@secondnode.hadoop.kr
The key's randomart image is:
+--[ DSA 1024]----+
|        .        |
|         o       |
|        o .      |
|       + o .     |
|      o S E      |
|     . = =.+     |
|        + *o+    |
|         =oo .   |
|        ..oo     |
+-----------------+

[hadoop@secondnode ~]$ mv ~/.ssh/id_dsa.pub ~/.ssh/secondnode.pub

[hadoop@secondnode ~]$ scp ~/.ssh/secondnode.pub  hadoop@namenode://home/hadoop/.ssh/secondnode.pub
Warning: Permanently added 'namenode,192.168.56.31' (ECDSA) to the list of known hosts.
hadoop@namenode's password:
secondnode.pub                             100%  617     0.6KB/s   00:00

[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@client ~]$
~~~


**클러스터 2번 머신 [ hd2-namenode ]**
~~~

[1]$ ssh namenode
[2]$ ll  .ssh
~~~

공개키(client, namenode, secondnode)들을 키박스에 추가
~~~
[3]$ cat   ~/.ssh/client.pub   >>  ~/.ssh/authorized_keys
     cat   ~/.ssh/namenode.pub   >>  ~/.ssh/authorized_keys
     cat   ~/.ssh/secondnode.pub   >>  ~/.ssh/authorized_keys
~~~

접근권한 변경 : 다른 사람이 보면 안되므로
~~~
[4]$ chmod   400   ~/.ssh/authorized_keys
[5]$ exit
~~~

~~~
[hadoop@client ~]$ ssh namenode
hadoop@namenode's password:
Last login: Thu Jun  1 17:37:54 2017 from client.hadoop.kr

[hadoop@namenode ~]$ ll .ssh
합계 20
-rw-r--r--. 1 hadoop hadoop 613  6월  1 17:35 client.pub
-rw-------. 1 hadoop hadoop 672  6월  1 17:38 id_dsa
-rw-r--r--. 1 hadoop hadoop 186  6월  1 17:41 known_hosts
-rw-r--r--. 1 hadoop hadoop 615  6월  1 17:38 namenode.pub
-rw-r--r--. 1 hadoop hadoop 617  6월  1 17:45 secondnode.pub

[hadoop@namenode ~]$ cat   ~/.ssh/client.pub   >>  ~/.ssh/authorized_keys
[hadoop@namenode ~]$ cat   ~/.ssh/namenode.pub   >>  ~/.ssh/authorized_keys
[hadoop@namenode ~]$ cat   ~/.ssh/secondnode.pub   >>  ~/.ssh/authorized_keys

[hadoop@namenode ~]$ chmod   400   ~/.ssh/authorized_keys

[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$
~~~

**클러스터 3번 머신 [ hd3-secondnode ]**

~~~
[1]$ ssh secondnode
[2]$ ll  .ssh
~~~

공개키(client, namenode, secondnode)들을 키박스에 추가
~~~
[3]$ cat   ~/.ssh/client.pub   >>  ~/.ssh/authorized_keys
     cat   ~/.ssh/namenode.pub   >>  ~/.ssh/authorized_keys
     cat   ~/.ssh/secondnode.pub   >>  ~/.ssh/authorized_keys
~~~

접근권한 변경 : 다른 사람이 보면 안되므로
~~~
[4]$ chmod   400   ~/.ssh/authorized_keys
[5]$ exit
~~~

~~~
[hadoop@client ~]$  ssh secondnode
hadoop@secondnode's password:
Last login: Thu Jun  1 17:50:50 2017 from client.hadoop.kr

[hadoop@secondnode ~]$ ll  .ssh
합계 20
-rw-r--r--. 1 hadoop hadoop 613  6월  1 17:52 client.pub
-rw-------. 1 hadoop hadoop 672  6월  1 17:44 id_dsa
-rw-r--r--. 1 hadoop hadoop 184  6월  1 17:45 known_hosts
-rw-r--r--. 1 hadoop hadoop 615  6월  1 17:41 namenode.pub
-rw-r--r--. 1 hadoop hadoop 617  6월  1 17:44 secondnode.pub

[hadoop@secondnode ~]$  cat   ~/.ssh/client.pub   >>  ~/.ssh/authorized_keys
[hadoop@secondnode ~]$  cat   ~/.ssh/namenode.pub   >>  ~/.ssh/authorized_keys
[hadoop@secondnode ~]$  cat   ~/.ssh/secondnode.pub   >>  ~/.ssh/authorized_keys

[hadoop@secondnode ~]$ chmod   400   ~/.ssh/authorized_keys

[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@client ~]$
~~~
=---> 비밀번호 없이 다른 머신에 접속하기 : 확인 작업( 중요 )

**클러스터 1번 머신 [ hd1-client ]**

- 2번과 3번 머신으로 ssh 접속 테스트
~~~
[1]$ ssh namenode
     w
     exit
[2]$ ssh secondnode
     w
     exit
~~~

~~~
[hadoop@client ~]$ ssh namenode
Last login: Thu Jun  1 17:48:33 2017 from client.hadoop.kr
[hadoop@namenode ~]$ w
 17:55:54 up  1:10,  2 users,  load average: 0.00, 0.04, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:46   20:34   0.07s  0.01s -bash
hadoop   pts/0    client.hadoop.kr 17:55    2.00s  0.01s  0.01s w
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.

[hadoop@client ~]$ ssh secondnode
Last login: Thu Jun  1 17:52:38 2017 from client.hadoop.kr
[hadoop@secondnode ~]$ w
 17:56:07 up  1:09,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:47    1:08m  0.04s  0.04s -bash
hadoop   pts/0    client.hadoop.kr 17:56    1.00s  0.01s  0.01s w
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@client ~]$
~~~

**클러스터 2번 머신 [ hd2-namenode ]**

=> 먼저 2번 머신으로 ssh 접속
~~~
[1]$ ssh namenode
~~~
- 자신과 3번 머신으로 ssh 접속 테스트
~~~
[2]$ ssh localhost
     w
     exit
[3]$ ssh namenode
     w
     exit
[4]$ ssh secondnode
     w
     exit
~~~
=> 2번 머신에서 로그아웃해서 다시 client 머신으로
~~~
[5]  exit
[6]  w
~~~

~~~
[hadoop@client ~]$ ssh namenode
Last login: Thu Jun  1 17:55:50 2017 from client.hadoop.kr
[hadoop@namenode ~]$ ssh localhost
Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
Last login: Thu Jun  1 17:59:22 2017 from client.hadoop.kr
[hadoop@namenode ~]$ w
 17:59:35 up  1:14,  3 users,  load average: 0.01, 0.03, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:46   24:15   0.07s  0.01s -bash
hadoop   pts/0    client.hadoop.kr 17:59    7.00s  0.02s  0.01s ssh localhos
hadoop   pts/1    localhost        17:59    6.00s  0.01s  0.01s w
[hadoop@namenode ~]$ exit
logout
Connection to localhost closed.
[hadoop@namenode ~]$ ssh namenode
Warning: Permanently added 'namenode,192.168.56.31' (ECDSA) to the list of known hosts.
Last login: Thu Jun  1 17:59:29 2017 from localhost
[hadoop@namenode ~]$ w
 17:59:57 up  1:14,  3 users,  load average: 0.01, 0.03, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:46   24:37   0.07s  0.01s -bash
hadoop   pts/0    client.hadoop.kr 17:59    5.00s  0.03s  0.02s ssh namenode
hadoop   pts/1    namenode.hadoop. 17:59    2.00s  0.01s  0.01s w
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@namenode ~]$ ssh secondnode
Last login: Thu Jun  1 17:56:06 2017 from client.hadoop.kr
[hadoop@secondnode ~]$ w
 18:00:14 up  1:13,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:47    1:12m  0.04s  0.04s -bash
hadoop   pts/0    namenode.hadoop. 18:00    2.00s  0.01s  0.01s w
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$ w
 18:00:21 up  1:05,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:55    1:05m  0.02s  0.02s -bash
root     pts/0    192.168.56.1     16:55    5.00s  0.06s  0.00s w
[hadoop@client ~]$
~~~

**클러스터 3번 머신 [ hd3-secondnode ]**

=> 먼저 3번 머신으로 ssh 접속
~~~
[1]$ ssh secondnode
~~~
자신과 2번 머신으로 ssh 접속 테스트
~~~
[2]$ ssh localhost
     w
     exit
[3]$ ssh namenode
     w
     exit
[4]$ ssh secondnode
     w
     exit
~~~
=> 3번 머신에서 로그아웃해서 다시 client 머신으로
~~~
[5]  exit
[6]  w
~~~

~~~
[hadoop@namenode ~]$ ssh secondnode
Last login: Thu Jun  1 17:56:06 2017 from client.hadoop.kr
[hadoop@secondnode ~]$ w
 18:00:14 up  1:13,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:47    1:12m  0.04s  0.04s -bash
hadoop   pts/0    namenode.hadoop. 18:00    2.00s  0.01s  0.01s w
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$ w
 18:00:21 up  1:05,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:55    1:05m  0.02s  0.02s -bash
root     pts/0    192.168.56.1     16:55    5.00s  0.06s  0.00s w
[hadoop@client ~]$
[hadoop@client ~]$
[hadoop@client ~]$
[hadoop@client ~]$ clear
[hadoop@client ~]$ ssh secondnode
Last login: Thu Jun  1 18:00:12 2017 from namenode.hadoop.kr
[hadoop@secondnode ~]$ ssh localhost
Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
Last login: Thu Jun  1 18:02:16 2017 from client.hadoop.kr
[hadoop@secondnode ~]$ w
 18:02:29 up  1:15,  3 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:47    1:14m  0.04s  0.04s -bash
hadoop   pts/0    client.hadoop.kr 18:02    5.00s  0.02s  0.02s ssh localhos
hadoop   pts/1    localhost        18:02    3.00s  0.01s  0.01s w
[hadoop@secondnode ~]$ exit
logout
Connection to localhost closed.
[hadoop@secondnode ~]$ ssh namenode
Last login: Thu Jun  1 17:59:55 2017 from namenode.hadoop.kr
[hadoop@namenode ~]$ w
 18:02:40 up  1:17,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:46   27:20   0.07s  0.01s -bash
hadoop   pts/0    secondnode.hadoo 18:02    0.00s  0.01s  0.01s w
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@secondnode ~]$ ssh secondnode
Warning: Permanently added 'secondnode,192.168.56.41' (ECDSA) to the list of known hosts.
Last login: Thu Jun  1 18:02:26 2017 from localhost
[hadoop@secondnode ~]$ w
 18:02:49 up  1:16,  3 users,  load average: 0.56, 0.13, 0.08
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:47    1:15m  0.04s  0.04s -bash
hadoop   pts/0    client.hadoop.kr 18:02    1.00s  0.02s  0.01s ssh secondno
hadoop   pts/1    secondnode.hadoo 18:02    1.00s  0.01s  0.01s w
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@client ~]$ w
 18:02:52 up  1:08,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:55    1:07m  0.02s  0.02s -bash
root     pts/0    192.168.56.1     16:55    4.00s  0.06s  0.00s w
[hadoop@client ~]$
~~~



#### (9) HDFS 포맷하기

##### HDFS 포맷

[NOTE] HDFS 포맷은 네임노드에서 해야 함

- 1번 머신에서 2번 머신으로 SSH 접속
~~~
[1]$ ssh namenode
~~~
- 호스트명 반드시 확인하세요!
~~~
[2]$ hostname
~~~

처음 한 번만 HDFS 포맷하기
~~~
[3]$ hadoop/bin/hdfs  namenode  -format
~~~

파일시스템 생성 확인
~~~
[4]$ ls /data
     ls -R /data
~~~

로그아웃하여 다시 1먼 머신으로
~~~
[5]$ exit
~~~

~~~
[hadoop@client ~]$ ssh namenode
Last login: Thu Jun  1 18:02:38 2017 from secondnode.hadoop.kr
[hadoop@namenode ~]$ hostname
namenode.hadoop.kr
[hadoop@namenode ~]$ hadoop/bin/hdfs  namenode  -format
17/06/07 14:34:02 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = namenode.hadoop.kr/192.168.56.31
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 2.6.4
...
...

17/06/07 14:34:05 INFO common.Storage: Storage directory /data/name1 has been successfully formatted.
17/06/07 14:34:05 INFO common.Storage: Storage directory /data/name2 has been successfully formatted.
17/06/07 14:34:05 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
17/06/07 14:34:05 INFO util.ExitUtil: Exiting with status 0
17/06/07 14:34:05 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at namenode.hadoop.kr/192.168.56.31
************************************************************/

[hadoop@namenode ~]$ ls /data
name1  name2

[hadoop@namenode ~]$ ls -R /data
/data:
name1  name2

/data/name1:
current

/data/name1/current:
VERSION                      fsimage_0000000000000000000.md5
fsimage_0000000000000000000  seen_txid

/data/name2:
current

/data/name2/current:
VERSION                      fsimage_0000000000000000000.md5
fsimage_0000000000000000000  seen_txid
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$
~~~
### (10) 하둡 서비스 시작하기
- HDFS, YARN, MapReduce
------------------------------------------------------------
------------------------------------------------------------


#### 1. HDFS 서비스 시작

~~~
[NOTE] HDFS 서비스 시작과 종료는 네임노드에서 해야 함

-> 1번 머신에서 2번 머신으로 SSH 접속
[1]$ ssh namenode

- 호스트명 반드시 확인하세요!
[2]$ hostname

- 자바 프로세스 조회
[3]$ jps

- HDFS 서비스 시작
[4]$ hadoop/sbin/start-dfs.sh

- 2번 머신에서 HDFS 자바 데몬 확인
[5]$ jps

-> 2번 머신에서 3번 머신으로 SSH 접속
[6]$ ssh secondnode

-> 3번 머신에서 HDFS 자바 데몬 확인
[7]$ jps

<- 로그아웃하여 다시 2번 머신으로
[8]$ exit

<- 로그아웃하여 다시 1번 머신으로
[9]$ exit
~~~

~~~
[hadoop@client ~]$ ssh namenode
Last login: Wed Jun  7 14:33:51 2017 from client.hadoop.kr
[hadoop@namenode ~]$ hostname
namenode.hadoop.kr
[hadoop@namenode ~]$ jps
1227 Jps
[hadoop@namenode ~]$ hadoop/sbin/start-dfs.sh
Starting namenodes on [namenode.hadoop.kr]
namenode.hadoop.kr: starting namenode, logging to /home/hadoop/hadoop/logs/hadoop-hadoop-namenode-namenode.hadoop.kr.out
datanode1.hadoop.kr: starting datanode, logging to /home/hadoop/hadoop/logs/hadoop-hadoop-datanode-namenode.hadoop.kr.out
datanode2.hadoop.kr: starting datanode, logging to /home/hadoop/hadoop/logs/hadoop-hadoop-datanode-secondnode.hadoop.kr.out
Starting secondary namenodes [secondnode.hadoop.kr]
secondnode.hadoop.kr: starting secondarynamenode, logging to /home/hadoop/hadoop/logs/hadoop-hadoop-secondarynamenode-secondnode.hadoop.kr.out
[hadoop@namenode ~]$ jps
2017 NameNode
2326 Jps
2139 DataNode
[hadoop@namenode ~]$ ssh secondnode
Last login: Thu Jun  1 18:02:47 2017 from secondnode.hadoop.kr
[hadoop@secondnode ~]$ jps
1294 Jps
1147 DataNode
1235 SecondaryNameNode
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$
~~~
--------------------------------------------------------

#### 2. YARN 서비스 시작

[NOTE] YARN 서비스 시작과 종료는 리소스매니저 노드에서 해야 함
~~~
-> 1번 머신에서 3번 머신으로 SSH 접속
[1]$ ssh secondnode

- 호스트명 반드시 확인하세요!
[2]$ hostname

- 자바 프로세스 조회
[3]$ jps

- YARN 서비스 시작
[4]$ hadoop/sbin/start-yarn.sh

- 3번 머신에서 YARN 자바 데몬 확인
[5]$ jps

-> 3번 머신에서 2번 머신으로 SSH 접속
[6]$ ssh namenode

-> 2번 머신에서 HDFS 자바 데몬 확인
[7]$ jps

<- 로그아웃하여 다시 3번 머신으로
[8]$ exit

<- 로그아웃하여 다시 1번 머신으로
[9]$ exit
~~~


~~~
[hadoop@client ~]$  ssh secondnode
Last login: Wed Jun  7 14:49:54 2017 from namenode.hadoop.kr
[hadoop@secondnode ~]$ hostname
secondnode.hadoop.kr
[hadoop@secondnode ~]$ jps
1424 Jps
1147 DataNode
1235 SecondaryNameNode
[hadoop@secondnode ~]$ hadoop/sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /home/hadoop/hadoop/logs/yarn-hadoop-resourcemanager-secondnode.hadoop.kr.out
datanode1.hadoop.kr: starting nodemanager, logging to /home/hadoop/hadoop/logs/yarn-hadoop-nodemanager-namenode.hadoop.kr.out
datanode2.hadoop.kr: starting nodemanager, logging to /home/hadoop/hadoop/logs/yarn-hadoop-nodemanager-secondnode.hadoop.kr.out

[hadoop@secondnode ~]$ jps
1700 Jps
1147 DataNode
1560 NodeManager
1235 SecondaryNameNode
1466 ResourceManager
[hadoop@secondnode ~]$ ssh namenode
Last login: Wed Jun  7 14:42:54 2017 from client.hadoop.kr
[hadoop@namenode ~]$ jps
2017 NameNode
2557 Jps
2427 NodeManager
2139 DataNode
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@client ~]$
~~~
#### 3. MapReduce 서비스 시작
~~~
[NOTE] 잡히스토리 서버 시작과 종료는 네임노드

-> 1번 머신에서 2번 머신으로 SSH 접속
[1]$ ssh namenode

- 호스트명 반드시 확인하세요!
[2]$ hostname

- 자바 프로세스 조회
[3]$ jps

- 잡히스토리 서버 시작
[4]$ hadoop/sbin/mr-jobhistory-daemon.sh start historyserver

- 2번 머신에서 잡히스토리 서버 데몬 확인
[5]$ jps

<- 로그아웃하여 다시 1번 머신으로
[6]$ exit
~~~
~~~
[hadoop@client ~]$ ssh namenode
Last login: Wed Jun  7 15:05:59 2017 from secondnode.hadoop.kr
[hadoop@namenode ~]$ hostname
namenode.hadoop.kr
[hadoop@namenode ~]$ jps
2017 NameNode
2427 NodeManager
2139 DataNode
2616 Jps
[hadoop@namenode ~]$ hadoop/sbin/mr-jobhistory-daemon.sh start historyserver
starting historyserver, logging to /home/hadoop/hadoop/logs/mapred-hadoop-historyserver-namenode.hadoop.kr.out


[hadoop@namenode ~]$ jps
2017 NameNode
2645 JobHistoryServer
2709 Jps
2427 NodeManager
2139 DataNode
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$
~~~
###
(11) HDFS 테스트하기

[NOTE] 하둡 HDFS, MapReduce 테스트는 클라이언트 노드에서
~~~
[1]$ hostname

[2]$  hadoop  fs  -lsr /
      hdfs dfs -chmod 777 /tmp
      hdfs dfs -mkdir /user
      hdfs dfs -mkdir /user/hadoop
      echo 'test' > test.txt
      hdfs dfs -put test.txt /user/hadoop/hadooptest.txt
      hdfs dfs -ls -R /user
      hdfs dfs -ls
~~~
~~~
[hadoop@client ~]$ hostname
client.hadoop.kr
[hadoop@client ~]$   hadoop  fs  -lsr /
lsr: DEPRECATED: Please use 'ls -R' instead.
drwxrwx---   - hadoop supergroup          0 2017-06-07 15:13 /tmp
drwxrwx---   - hadoop supergroup          0 2017-06-07 15:13 /tmp/hadoop-yarn
drwxrwx---   - hadoop supergroup          0 2017-06-07 15:13 /tmp/hadoop-yarn/staging
drwxrwx---   - hadoop supergroup          0 2017-06-07 15:13 /tmp/hadoop-yarn/staging/history
drwxrwx---   - hadoop supergroup          0 2017-06-07 15:13 /tmp/hadoop-yarn/staging/history/done
drwxrwxrwt   - hadoop supergroup          0 2017-06-07 15:13 /tmp/hadoop-yarn/staging/history/done_intermediate

[hadoop@client ~]$       hdfs dfs -chmod 777 /tmp
[hadoop@client ~]$       hdfs dfs -mkdir /user
[hadoop@client ~]$       hdfs dfs -mkdir /user/hadoop
[hadoop@client ~]$       echo 'test' > test.txt
[hadoop@client ~]$       hdfs dfs -put test.txt /user/hadoop/hadooptest.txt
[hadoop@client ~]$       hdfs dfs -ls -R /user
drwxr-xr-x   - hadoop supergroup          0 2017-06-07 15:20 /user/hadoop
-rw-r--r--   1 hadoop supergroup          5 2017-06-07 15:20 /user/hadoop/hadooptest.txt
[hadoop@client ~]$       hdfs dfs -ls
Found 1 items
-rw-r--r--   1 hadoop supergroup          5 2017-06-07 15:20 hadooptest.txt
[hadoop@client ~]$
~~~
(12) Pig로 테스트하기


[NOTE] Pig는 클라이언트 머신에서 실행함
~~~
[1]$ hostname
~~~
- Pig 설치 확인
~~~
[2]$ ls
~~~
- 테스트용 파일 만들어 HDFS에 올리기
~~~
[3]$ echo 'aaa,100' > pig.txt
     echo 'bbb,200' >> pig.txt
     echo 'ccc,300' >> pig.txt
     echo 'aaa,400' >> pig.txt

[4]$ hdfs dfs -put  pig.txt  /user/hadoop/pig.txt

[5]$ hdfs dfs -ls  .

[6]$ hdfs dfs -cat   pig.txt
~~~
- Pig의 CLI인 Grunt 쉘에 접속하기
~~~
[7]$ pig
~~~
~~~
[hadoop@client ~]$ hostname
client.hadoop.kr
[hadoop@client ~]$ ls
apache-hive-1.2.1-bin  mysql-connector-java-5.1.23-bin.jar
bashrc2.txt            org_bashrc
etc_hadoop_2017        pig
hadoop                 pig-0.16.0
hadoop-2.6.4           spark
hdconf_2017.tgz        spark-1.6.2-bin-hadoop2.6
hive                   test.txt
hive-1.2.1-site.xml
[hadoop@client ~]$ echo 'aaa,100' > pig.txt
[hadoop@client ~]$      echo 'bbb,200' >> pig.txt
[hadoop@client ~]$      echo 'ccc,300' >> pig.txt
[hadoop@client ~]$      echo 'aaa,400' >> pig.txt
[hadoop@client ~]$ hdfs dfs -put  pig.txt  /user/hadoop/pig.txt
[hadoop@client ~]$ hdfs dfs -ls  .
Found 2 items
-rw-r--r--   1 hadoop supergroup          5 2017-06-07 15:20 hadooptest.txt
-rw-r--r--   1 hadoop supergroup         32 2017-06-07 15:26 pig.txt
[hadoop@client ~]$ hdfs dfs -cat   pig.txt
aaa,100
bbb,200
ccc,300
aaa,400
[hadoop@client ~]$ pig
....
....
grunt>
~~~

**Grunt 쉘에서 테스트하기**
~~~
grunt> ls
grunt> cat  pig.txt
grunt> k2 = load 'pig.txt' using PigStorage(',')
                  as ( str:chararray, price:int );
grunt> k4 = GROUP k2 BY $0;
grunt> k8 = foreach k4 generate group, SUM(k2.price);
grunt> dump  k8;
grunt> store  k8  into 'mr_pig';
grunt> cat  mr_pig
grunt> illustrate  k8;
grunt> quit
~~~


~~~
grunt> ls
hdfs://namenode.hadoop.kr:8020/user/hadoop/hadooptest.txt<r 1>  5
hdfs://namenode.hadoop.kr:8020/user/hadoop/pig.txt<r 1> 32

grunt> cat  pig.txt
aaa,100
bbb,200
ccc,300
aaa,400

grunt> k2 = load 'pig.txt' using PigStorage(',')
>>                   as ( str:chararray, price:int );
2017-05-31 13:28:00,902 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS

grunt> k4 = GROUP k2 BY $0;

grunt> k8 = foreach k4 generate group, SUM(k2.price);

grunt> dump  k8;
...
[ MapReduce가 실행됨 => 결과값을 바로 확인 ]
...
(aaa,500)
(bbb,200)
(ccc,300)

grunt> store  k8  into 'mr_pig';
...
[ MapReduce가 실행됨 => 'mr_pig'에 결과값 저장 ]
...
2017-05-31 13:35:15,901 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!

grunt> cat  mr_pig
aaa     500
bbb     200
ccc     300

grunt> illustrate  k8;
--------------------------------------------
| k2     | str:chararray    | price:int    |
--------------------------------------------
|        | aaa              | 400          |
|        | aaa              | 100          |
--------------------------------------------
---------------------------------------------------------------------------------------
| k4     | group:chararray    | k2:bag{:tuple(str:chararray,price:int)}               |
---------------------------------------------------------------------------------------
|        | aaa                | {}                                                    |
|        | aaa                | {}                                                    |
---------------------------------------------------------------------------------------
-------------------------------------------
| k8     | group:chararray    | :long     |
-------------------------------------------
|        | aaa                | 500       |
-------------------------------------------

grunt> quit;
2017-05-31 13:38:13,071 [main] INFO  org.apache.pig.Main - Pig script completed in 12 minutes, 28 seconds and 538 milliseconds (748538 ms)
[hadoop@client ~]$
~~~


* pig의 grunt>에 접속해 있을 때
  1번 머신 = 클라이언트 노드의 jps 결과
~~~
[hadoop@client ~]$ jps
1813 Jps
1705 RunJar => pig의 grunt> 쉘
[hadoop@client ~]$
~~~

* pig의 grunt>에서 맵리듀스를 실행할 때
  2번 머신 = 네임노드의 jps 결과
~~~
[hadoop@client ~]$ ssh namenode
Last login: Wed Jun  7 15:47:50 2017 from client.hadoop.kr

[hadoop@namenode ~]$ jps
2017 NameNode
3747 MRAppMaster       => 맵리듀스 어플리케이션 마스터
2645 JobHistoryServer
3917 Jps
3878 YarnChild         => 맵리듀스 태스크 콘테이너
2427 NodeManager
2139 DataNode
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$
~~~
### (13) 하둡 서비스 중단하기

**하둡 서비스 중단하기**

=> 서비스 시작 순서
    HDFS -> YARN -> MR-History Server

=> 서비스 중단하기 순서
    YARN -> MR-History Server -> HDFS

[NOTE] HDFS 서비스를 처음 시작하고 마지막에 종료


-> 1번 머신에서 3번 머신으로 SSH 접속

[1]$ ssh secondnode

- YARN 서비스 중단하기

[2]$ hadoop/sbin/stop-yarn.sh

-> 3번 머신에서 자바 데몬 확인

[3]$ jps

-> 3번 머신에서 2번 머신으로 SSH 접속

[4]$ ssh namenode

- MR-History Server 중단하기

[5]$ hadoop/sbin/mr-jobhistory-daemon.sh stop historyserver

- HDFS 서비스 중단하기

[6]$ hadoop/sbin/stop-dfs.sh

-> 2번 머신에서 HDFS 자바 데몬 확인
[7]$ jps

<- 로그아웃하여 다시 3번 머신으로
[8]$ exit

<- 로그아웃하여 다시 1번 머신으로
[9]$ exit
~~~
[hadoop@client ~]$ ssh secondnode
Last login: Wed Jun  7 15:47:23 2017 from namenode.hadoop.kr
[hadoop@secondnode ~]$ hadoop/sbin/stop-yarn.sh
stopping yarn daemons
stopping resourcemanager
datanode2.hadoop.kr: stopping nodemanager
datanode1.hadoop.kr: stopping nodemanager
datanode2.hadoop.kr: nodemanager did not stop gracefully after 5 seconds: killing with kill -9
datanode1.hadoop.kr: nodemanager did not stop gracefully after 5 seconds: killing with kill -9
no proxyserver to stop
[hadoop@secondnode ~]$ jps
1147 DataNode
1235 SecondaryNameNode
2686 Jps
[hadoop@secondnode ~]$ ssh namenode
Last login: Wed Jun  7 15:50:09 2017 from client.hadoop.kr
[hadoop@namenode ~]$ hadoop/sbin/mr-jobhistory-daemon.sh stop historyserver
stopping historyserver
[hadoop@namenode ~]$ hadoop/sbin/stop-dfs.sh
Stopping namenodes on [namenode.hadoop.kr]
namenode.hadoop.kr: stopping namenode
datanode2.hadoop.kr: stopping datanode
datanode1.hadoop.kr: stopping datanode
Stopping secondary namenodes [secondnode.hadoop.kr]
secondnode.hadoop.kr: stopping secondarynamenode
[hadoop@namenode ~]$ jps
4425 Jps
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@client ~]$

~~~

# 하둡 기본 명령어
# 샘플 데이터
