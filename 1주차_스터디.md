### 목차
* [하둡이란](#하둡이란)
* [데이터 분석 아키텍쳐](#데이터-분석-아키텍쳐)
* [실습](#실)
  * [다운로드](#다운로드)
    + [Virtual Box 다운로드](#Virtual-box-다운로드)
    + [CentOS 7 다운로드](#centos-7-다운로드)
    + [putty 다운로드](#putty-다운로드)
* [Virtual Box 이미지 올리고 환경 설정](#Virtual-Box-이미지-올리고-환경-설정)
* [Virtual Box 가상 머신 콘솔 실행](#Virtual-Box-가상-머신-콘솔-실행)
  - [가상머신 콘솔에서 고정 IP 설정](#가상머신-콘솔에서-고정-IP-설정)
  - [가상머신에 ssh 접속](#가상머신에-ssh-접속)
  - [필요한 리눅스 프로그램 설치](#필요한-리눅스-프로그램-설치)
  - [JAVA 다운로드 및 설치](#JAVA-다운로드-및-설치)
  - [가상 머신 중단 및 VDI 이미지 백업하기](#가상-머신-중단-및-VDI-이미지-백업하기)
  - [VDI 파일 백업하기](#VDI-파일-백업하기)

## 하둡이란
-Apache Hadoop은 빅데이터를 저장, 처리, 분석할 수 있는 소프트웨어 프레임워크입니다.
- http://12bme.tistory.com/70
- https://ko.wikipedia.org/wiki/%EC%95%84%ED%8C%8C%EC%B9%98_%ED%95%98%EB%91%A1
- http://yookeun.github.io/java/2015/05/24/hadoop-hdfs/

## 데이터 분석 아키텍쳐
- 수집 -> 적재 -> 처리/탐색 -> 분석/응용
- [참고이미지](https://m.blog.naver.com/PostView.nhn?blogId=samsjang&logNo=220788844868&proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F)
#### 이번 실습에서의 필요 사양 (학습/테스트 용도)
- CPU : 듀얼코어 2.93Gz
- 램 : 2.0GB
- 하드디스크 : 100GB
- OS : CentOS 6 이상


#### [참고] 빅데이터 파일럿 프로젝트 시, 필요 사양
- 다양한 하둡 에코 시스템을 사용하는 파일럿 프로젝트를 사용할 때의 사양
- 메모리에 대한 리소스 사용률이 높으므로 여유 메모리를 최대한 확보

리소스 구분  | 저사양 파일럿 환경 | 고사양환경 | 비고
--------- | -------------- | ------- | ----
CPU | 듀얼코어 이상 | i5 이상 | i3 이상 권장
메모리| 8GB 이상 (여유 7GB) | 16GB 이상(여유 15GB) | 16GB 이상 권장
디스크 | 60GB 이상 | 100GB 이상 | SSD 권장


- OS는 CentOS 권장 (오픈소스 표준)

#### Virtual Box
- 가상머신이란?
- 가상머신을 왜 사용하는가?
- 대표적인 가상머신으로는 **Virtual Box**와 **VMWare**가 있다

## 실습
### 다운로드
#### Oracle Virtual Box 다운로드
- https://www.virtualbox.org/wiki/Downloads

#### CentOS 7 iso 다운로드
- http://mirror.retentionrange.co.bw/centOS/7/isos/x86_64/

#### putty 다운로드
- https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html

### Virtual Box 이미지 올리고 환경 설정
- PPT 참고

### Virtual Box 가상 머신 콘솔 실행
#### 인터넷 연결 확인
~~~
$ ping -c 2 www.google.com
~~~
#### 가상머신 콘솔에서 고정 IP 설정
- 가상머신 콘솔에서 root로 로그인

~~~
client login: root
Password:  [hadoop]입력해주세요
~~~

- ifconfig 설치

~~~
[1]$ yum  install  -y  net-tools
~~~
~~~
[root@client ~]# yum install -y net-tools
Loaded plugins: fastestmirror
...
Dependencies Resolved
================================================================================
 Package         Arch         Version                          Repository  Size
================================================================================
Installing:
 net-tools       x86_64       2.0-0.17.20131004git.el7         base       304 k

Transaction Summary
================================================================================
...
Running transaction
  Installing : net-tools-2.0-0.17.20131004git.el7.x86_64                    1/1
  Verifying  : net-tools-2.0-0.17.20131004git.el7.x86_64                    1/1

Installed:
  net-tools.x86_64 0:2.0-0.17.20131004git.el7

Complete!
-----------------------------------------------------------------------------------
~~~
-  먼저 IP 설정을 확인하고 디바이스 이름( enp... 로 시작) 확인

~~~
[2]$ ip a
~~~
~~~
[root@client ~]# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 12:34:56:78:90:20 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3
       valid_lft 73401sec preferred_lft 73401sec
    inet6 fe80::de96:c091:e0e0:a35f/64 scope link
       valid_lft forever preferred_lft forever
3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 12:34:56:78:90:21 brd ff:ff:ff:ff:ff:ff
    inet 192.168.56.101/24 brd 192.168.56.255 scope global dynamic enp0s8
       valid_lft 1099sec preferred_lft 1099sec
    inet6 fe80::58f4:a9a4:185b:2f73/64 scope link
       valid_lft forever preferred_lft forever
~~~

[ 3: enp0s8 ] 디바이스에 [ inet 192.168.56.101/24 ] 확인
                         (192.168.56.1XX)

IP를 고정시켜 줌
~~~
[3]$ ifconfig  enp0s8  192.168.56.21
[4]$ ip  a
~~~
부팅시 자동으로 실행되도록 rc.local에 추가

// 지금 실행하면 안됩니다. echo 'ifconfig  enp0s8  192.168.56.21' >> /etc/rc.d/rc.local

~~~
[root@client ~]# ifconfig  enp0s8  192.168.56.21

[root@client ~]# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 12:34:56:78:90:20 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3
       valid_lft 73183sec preferred_lft 73183sec
    inet6 fe80::de96:c091:e0e0:a35f/64 scope link
       valid_lft forever preferred_lft forever
3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 12:34:56:78:90:21 brd ff:ff:ff:ff:ff:ff
    inet 192.168.56.21/24 brd 192.168.56.255 scope global enp0s8
       valid_lft forever preferred_lft forever
    inet6 fe80::58f4:a9a4:185b:2f73/64 scope link
       valid_lft forever preferred_lft forever
~~~

출력 화면에서 다음과 같이 IP 설정된 것을 확인해야 함

~~~
1: log    => 127.0.0.1
2: enpXXX => 10.0.2.15
3: enpXXX => 192.168.56.21
~~~


#### 가상머신에 ssh 접속 (Putty)
~~~
IP : 192.168.56.21
~~~  
외부 네트워크 연결 확인하기
~~~
[1]$ hostname
[2]$ ping -c 2 www.google.com
~~~
~~~
//  네트워크 연결이 안될 경우!
[NOTE] 먼저 노트북/PC의 인터넷 연결 여부를 확인하세요.
[root@client ~]# echo 'nameserver 8.8.8.8' >> /etc/resolv.conf
// 실행 후, 다시 ping -c 2 www.google.com
~~~

~~~
login as: root
root@192.168.56.21's password:
Last login: Fri May 26 18:13:00 2017 from 192.168.56.1

[root@client ~]# hostname
client.hadoop.kr

[root@client ~]# ping -c 2 www.google.com
PING www.google.com (216.58.197.4) 56(84) bytes of data.
64 bytes from kix06s02-in-f4.1e100.net (216.58.197.4): icmp_seq=1 ttl=51 time=45.7 ms
64 bytes from kix06s02-in-f4.1e100.net (216.58.197.4): icmp_seq=2 ttl=51 time=39.5 ms

--- www.google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 39.538/42.639/45.740/3.101 ms
~~~
#### 필요한 리눅스 프로그램 설치
- ftp, wget 프로그램 설치
~~~
[3]$ yum install -y ftp wget
~~~
~~~
[root@client ~]# rpm -qa | grep wget
[root@client ~]# rpm -qa | grep ftp
[root@client ~]# yum install -y ftp wget
Loaded plugins: fastestmirror
...
  Installing : ftp-0.17-67.el7.x86_64                                                                   1/2
  Installing : wget-1.14-13.el7.x86_64                                                                  2/2
  Verifying  : wget-1.14-13.el7.x86_64                                                                  1/2
  Verifying  : ftp-0.17-67.el7.x86_64                                                                   2/2

Installed:
  ftp.x86_64 0:0.17-67.el7         wget.x86_64 0:1.14-13.el7

Complete!
~~~

Mysql 설치
- 마리아DB 대신 Mysql 설치할 수 있게 해주는 패키지 다운로드
~~~
[4]$ wget www.db21.co.kr/big/mysql-community-release-el7-5.noarch.rpm
~~~
패키지 설치
~~~
[5]$ yum -y install mysql-community-release-el7-5.noarch.rpm
~~~

Mysql 설치
~~~
[6]$ yum -y install mysql-community-server
~~~

Mysql 서버 시작
~~~
[7]$ systemctl start mysqld
~~~

(옵선) 리눅스 부팅시 Mysql 서버 자동 시작 설정(일단 하지 X)
~~~
[8]$ s y s t e m c t l enable mysql
~~~

설정 확인
~~~
[9]$ systemctl status mysqld
~~~
Mysql 서버 동작 확인
~~~
[10] $ mysqlshow
~~~

~~~
[root@client ~]# wget www.db21.co.kr/big/mysql-community-release-el7-5.noarch.rpm
--2017-05-26 18:51:17--  http://www.db21.co.kr/big/mysql-community-release-el7-5.noarch.rpm
Resolving www.db21.co.kr (www.db21.co.kr)... 121.124.124.244
Connecting to www.db21.co.kr (www.db21.co.kr)|121.124.124.244|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 6140 (6.0K) [text/plain]
Saving to: ‘mysql-community-release-el7-5.noarch.rpm’

100%[==================================================================>] 6,140       --.-K/s   in 0.004s

2017-05-26 18:51:17 (1.38 MB/s) - ‘mysql-community-release-el7-5.noarch.rpm’ saved [6140/6140]

[root@client ~]# yum -y install mysql-community-release-el7-5.noarch.rpm
Loaded plugins: fastestmirror
...
Running transaction
  Installing : mysql-community-release-el7-5.noarch                                                     1/1
  Verifying  : mysql-community-release-el7-5.noarch                                                     1/1

Installed:
  mysql-community-release.noarch 0:el7-5

Complete!

[root@client ~]# yum -y install mysql-community-server
Loaded plugins: fastestmirror

Installed:
  mysql-community-libs.x86_64 0:5.6.36-2.el7          mysql-community-server.x86_64 0:5.6.36-2.el7

Dependency Installed:
  libaio.x86_64 0:0.3.109-13.el7                       mysql-community-client.x86_64 0:5.6.36-2.el7
  mysql-community-common.x86_64 0:5.6.36-2.el7         perl.x86_64 4:5.16.3-291.el7
...
  perl-threads-shared.x86_64 0:1.43-6.el7

Replaced:
  mariadb-libs.x86_64 1:5.5.52-1.el7

Complete!
~~~

~~~
[root@client ~]# systemctl start mysqld

[root@client ~]# systemctl enable mysqld

[root@client ~]# systemctl status mysqld
● mysqld.service - MySQL Community Server
   Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled)
   Active: active (running) since 금 2017-05-26 18:54:20 KST; 11s ago
 Main PID: 8901 (mysqld_safe)
   CGroup: /system.slice/mysqld.service
           ├─8901 /bin/sh /usr/bin/mysqld_safe --basedir=/usr
           └─9068 /usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib64/mysql/...

 5월 26 18:54:19 client.hadoop.kr mysql-systemd-start[8842]: Support MySQL by buying support/licenses ...om
 ...
 5월 26 18:54:20 client.hadoop.kr systemd[1]: Started MySQL Community Server.
Hint: Some lines were ellipsized, use -l to show in full.

[root@client ~]# mysqlshow
+--------------------+
|     Databases      |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
+--------------------+
~~~

#### JAVA 다운로드 및 설치
- JAVA 다운로드(64비트)
~~~
[11]$ wget www.db21.co.kr/hadoop/jdk-7u55-linux-x64.rpm

[root@client ~]# wget www.db21.co.kr/hadoop/jdk-7u55-linux-x64.rpm
--2017-05-26 18:26:00--  http://www.db21.co.kr/hadoop/jdk-7u55-linux-x64.rpm
Resolving www.db21.co.kr (www.db21.co.kr)... 121.124.124.244
Connecting to www.db21.co.kr (www.db21.co.kr)|121.124.124.244|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 122656363 (117M) [text/plain]
Saving to: ‘jdk-7u55-linux-x64.rpm’

100%[===============================================>] 122,656,363 2.17MB/s   in 57s

2017-05-26 18:26:58 (2.04 MB/s) - ‘jdk-7u55-linux-x64.rpm’ saved [122656363/122656363]
~~~

**JAVA 설치**

[NOTE] 하둡계정은 별도의 JDK 버전을 사용함
- JDK의 설치 여부를 확인

~~~
[12]$ rpm -qa | grep 'jdk'
=> 아무것도 안나오면 정상!

[13]$ ls
=> JDK 설치

[14]$ rpm  -ivh  jdk-7u55-linux-x64.rpm
=> 심벌릭 링크를 걸어줌(버전 업그레이드할 때 편리함)

[15]$ ln  -s  /usr/java/jdk1.7.0_55  /usr/local/java

[16]$ ll /usr/local
~~~

~~~
[root@client ~]# rpm -qa | grep 'jdk'

[root@client ~]# ls
anaconda-ks.cfg  jdk-7u55-linux-x64.rpm  mysql-community-release-el7-5.noarch.rpm

[root@client ~]# rpm  -ivh  jdk-7u55-linux-x64.rpm
준비 중...                         ################################# [100%]
Updating / installing...
   1:jdk-2000:1.7.0_55-fcs            ################################# [100%]
Unpacking JAR files...
        rt.jar...
        jsse.jar...
        charsets.jar...
        tools.jar...
        localedata.jar...
        jfxrt.jar...

[root@client ~]# ln  -s  /usr/java/jdk1.7.0_55  /usr/local/java

[root@client ~]# ll /usr/local
합계 0
drwxr-xr-x. 2 root root  6 11월  6  2016 bin
drwxr-xr-x. 2 root root  6 11월  6  2016 etc
drwxr-xr-x. 2 root root  6 11월  6  2016 games
drwxr-xr-x. 2 root root  6 11월  6  2016 include
lrwxrwxrwx. 1 root root 21  5월 26 19:01 java -> /usr/java/jdk1.7.0_55
drwxr-xr-x. 2 root root  6 11월  6  2016 lib
drwxr-xr-x. 2 root root  6 11월  6  2016 lib64
drwxr-xr-x. 2 root root  6 11월  6  2016 libexec
drwxr-xr-x. 2 root root  6 11월  6  2016 sbin
drwxr-xr-x. 5 root root 49  5월 26 14:33 share
drwxr-xr-x. 2 root root  6 11월  6  2016 src
------------------------------------------------------------
~~~

**추가 작업 및 설정**

1.방화벽 중단하기
~~~
$ systemctl status firewalld

$ systemctl stop firewalld

$ systemctl disable firewalld
~~~
2.JDK RPM 파일 삭제
~~~
$ ll
$ rm jdk-7u55-linux-x64.rpm
~~~

**하둡 설치 관련 파일 다운로드**

- 리눅스 하둡 계정(hadoop) 추가
- 비밀번호는 hadoop
~~~
[1]$ adduser hadoop
[2]$ passwd hadoop
~~~

~~~
[root@client ~]# adduser hadoop
[root@client ~]# passwd hadoop
hadoop 사용자의 비밀 번호 변경 중
새  암호:
잘못된 암호: 암호는 8 개의 문자 보다 짧습니다
새  암호 재입력:
passwd: 모든 인증 토큰이 성공적으로 업데이트 되었습니다.
~~~

- 하둡 데이터 디렉터리 만들고 권한 부여

~~~
[3]$ mkdir   /data
[4]$ chown  hadoop  /data
[5]$ chgrp  hadoop  /data
[6]$ ll  /
~~~

~~~
[root@client ~]# mkdir  /data
[root@client ~]# chown  hadoop  /data
[root@client ~]# chgrp  Hadoop  /data
[root@client ~]# ll /
합계 16
lrwxrwxrwx.   1 root   root      7  5월 31 14:43 bin -> usr/bin
dr-xr-xr-x.   4 root   root   4096  5월 31 14:46 boot
drwxr-xr-x.   2 hadoop hadoop    6  5월 31 15:47 data
drwxr-xr-x.  17 root   root   2980  5월 31 14:50 dev
drwxr-xr-x.  76 root   root   8192  5월 31 15:47 etc
drwxr-xr-x.   3 root   root     20  5월 31 15:47 home
lrwxrwxrwx.   1 root   root      7  5월 31 14:43 lib -> usr/lib
lrwxrwxrwx.   1 root   root      9  5월 31 14:43 lib64 -> usr/lib64
drwxr-xr-x.   2 root   root      6 11월  6  2016 media
drwxr-xr-x.   2 root   root      6 11월  6  2016 mnt
drwxr-xr-x.   2 root   root      6 11월  6  2016 opt
dr-xr-xr-x. 103 root   root      0  5월 31 14:50 proc
dr-xr-x---.   2 root   root    192  5월 31 15:46 root
drwxr-xr-x.  22 root   root    640  5월 31 15:44 run
lrwxrwxrwx.   1 root   root      8  5월 31 14:43 sbin -> usr/sbin
drwxr-xr-x.   2 root   root      6 11월  6  2016 srv
dr-xr-xr-x.  13 root   root      0  5월 31 14:50 sys
drwxrwxrwt.   7 root   root    132  5월 31 15:46 tmp
drwxr-xr-x.  14 root   root    167  5월 31 15:47 usr
drwxr-xr-x.  19 root   root    267  5월 31 14:50 var
~~~

**SSH 처음 접속시 질문안받기**


~~~
[7]$ echo 'StrictHostKeyChecking no' >> /etc/ssh/ssh_config
~~~

[NOTE] 나중에 IP 설정이 꼬이면 아예 접속이 안됨
       이때는 ~/.ssh/known_hosts 삭제하면 됨!!!
       ( /root 또는 /home/hadoop )
~~~
[8]$ tail  /etc/ssh/ssh_config
~~~


~~~
[root@client ~]# echo 'StrictHostKeyChecking no' >> /etc/ssh/ssh_config

[root@client ~]# tail  /etc/ssh/ssh_config
# If this option is set to yes then remote X11 clients will have full access
# to the original X11 display. As virtually no X11 client supports the untrusted
# mode correctly we set this to yes.
        ForwardX11Trusted yes
# Send locale-related environment variables
        SendEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES
        SendEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT
        SendEnv LC_IDENTIFICATION LC_ALL LANGUAGE
        SendEnv XMODIFIERS
StrictHostKeyChecking no
[root@client ~]#
~~~

**하둡 계정으로 로그인하여 필요한 파일 다운로드**
~~~
[9]$ su  -  hadoop
~~~
[NOTE] 띄어쓰기 조심 : "su" 한칸 "-"(마이너스) 한칸 "hadoop"

=> Prompt가 다음과 같이 변경된 것을 확인하세요.
~~~
[root@client ~]# su  -  hadoop
[hadoop@client ~]$
~~~


=> 이제부터는 root가 아닌 hadoop 계정으로 작업합니다.
~~~
[1]$ whoami
     pwd
~~~
~~~
[hadoop@client ~]$ whoami
hadoop
[hadoop@client ~]$ pwd
/home/hadoop
~~~

**필요한 하둡 관련 파일 다운로드**
~~~
[2]$ wget www.db21.co.kr/big/hadoop-2.6.4.tar.gz
     wget www.db21.co.kr/big/bashrc2.txt
     wget www.db21.co.kr/big/pig-0.16.0.tar.gz
     wget www.db21.co.kr/big/apache-hive-1.2.1-bin.tar.gz
     wget www.db21.co.kr/big/hive-1.2.1-site.xml
     wget www.db21.co.kr/big/mysql-connector-java-5.1.23-bin.jar
     wget www.db21.co.kr/big/spark-1.6.2-bin-hadoop2.6.tgz
~~~
~~~
[hadoop@client ~]$ wget www.db21.co.kr/big/hadoop-2.6.4.tar.gz
--2017-05-26 18:32:05--  http://www.db21.co.kr/big/hadoop-2.6.4.tar.gz
Resolving www.db21.co.kr (www.db21.co.kr)...     

...
...

Saving to: ‘spark-1.6.2-bin-hadoop2.6.tgz’
100%[=====================================>] 278,057,117 4.51MB/s   in 71s
2017-05-26 18:35:13 (3.73 MB/s) - ‘spark-1.6.2-bin-hadoop2.6.tgz’ saved [278057117/278057117]
~~~

==> Hadoop, Pig, Hive, Spark 압축풀고 링크 걸기


##### 하둡 압축풀기

하둡 Tarball 파일 압축 풀기
~~~
[3]$ tar xvzf hadoop-2.6.4.tar.gz
~~~

버전 관리를 위해서 심벌릭 링크 걸기

~~~
[4]$ ln -s  /home/hadoop/hadoop-2.6.4  /home/hadoop/hadoop
~~~

##### Pig 압축풀기

Pig Tarball 파일 압축 풀기
~~~
[5]$ tar xvzf  pig-0.16.0.tar.gz
~~~

버전 관리를 위해서 심벌릭 링크 걸기
~~~
[6]$ ln -s  pig-0.16.0  pig
~~~

##### Hive 압축풀기

Hive Tarball 파일 압축 풀기
~~~
[7]$ tar xvzf apache-hive-1.2.1-bin.tar.gz
~~~
버전 관리를 위해서 심벌릭 링크 걸기
~~~
[8]$ ln -s apache-hive-1.2.1-bin  hive
~~~

##### Spark 압축풀기

- Spark Tarball 파일 압축 풀기
~~~
[9]$ tar xvzf  spark-1.6.2-bin-hadoop2.6.tgz
[10]$ ln -s  spark-1.6.2-bin-hadoop2.6  spark
~~~

##### 확인하기

~~~
[11]$ ll
~~~

~~~
[hadoop@client ~]$ ll
합계 727756
drwxrwxr-x.  8 hadoop hadoop       159  5월 26 19:07 apache-hive-1.2.1-bin
-rw-rw-r--.  1 hadoop hadoop  92834839  6월 27  2015 apache-hive-1.2.1-bin.tar.gz
-rw-rw-r--.  1 hadoop hadoop      1512  9월 17  2016 bashrc2.txt
lrwxrwxrwx.  1 hadoop hadoop        25  5월 26 19:07 hadoop -> /home/hadoop/hadoop-2.6.4
drwxr-xr-x.  9 hadoop hadoop       149  2월 12  2016 hadoop-2.6.4
-rw-rw-r--.  1 hadoop hadoop 196015975  2월 12  2016 hadoop-2.6.4.tar.gz
lrwxrwxrwx.  1 hadoop hadoop        21  5월 26 19:07 hive -> apache-hive-1.2.1-bin
-rw-rw-r--.  1 hadoop hadoop    168409  9월 17  2016 hive-1.2.1-site.xml
-rw-rw-r--.  1 hadoop hadoop    843090  3월 24  2013 mysql-connector-java-5.1.23-bin.jar
lrwxrwxrwx.  1 hadoop hadoop        10  5월 26 19:07 pig -> pig-0.16.0
drwxr-xr-x. 16 hadoop hadoop      4096  6월  2  2016 pig-0.16.0
-rw-rw-r--.  1 hadoop hadoop 177279333  6월  8  2016 pig-0.16.0.tar.gz
lrwxrwxrwx.  1 hadoop hadoop        25  5월 26 19:08 spark -> spark-1.6.2-bin-hadoop2.6
drwxr-xr-x. 12 hadoop hadoop       210  6월 22  2016 spark-1.6.2-bin-hadoop2.6
-rw-rw-r--.  1 hadoop hadoop 278057117  6월 25  2016 spark-1.6.2-bin-hadoop2.6.tgz

~~~

##### Tarball 파일 삭제

4개의 Tarball 파일 삭제
~~~
[12]$ rm  hadoop-2.6.4.tar.gz                  
      rm  pig-0.16.0.tar.gz
      rm  apache-hive-1.2.1-bin.tar.gz  
      rm  spark-1.6.2-bin-hadoop2.6.tgz
~~~

#####확인
~~~
[13]$ ll
~~~
~~~
[hadoop@client ~]$ rm  hadoop-2.6.4.tar.gz
[hadoop@client ~]$ rm  pig-0.16.0.tar.gz
[hadoop@client ~]$ rm  apache-hive-1.2.1-bin.tar.gz
[hadoop@client ~]$ rm  spark-1.6.2-bin-hadoop2.6.tgz
[hadoop@client ~]$ ll
합계 1000
drwxrwxr-x.  8 hadoop hadoop    159  5월 26 19:07 apache-hive-1.2.1-bin
-rw-rw-r--.  1 hadoop hadoop   1512  9월 17  2016 bashrc2.txt
lrwxrwxrwx.  1 hadoop hadoop     25  5월 26 19:07 hadoop -> /home/hadoop/hadoop-2.6.4
drwxr-xr-x.  9 hadoop hadoop    149  2월 12  2016 hadoop-2.6.4
lrwxrwxrwx.  1 hadoop hadoop     21  5월 26 19:07 hive -> apache-hive-1.2.1-bin
-rw-rw-r--.  1 hadoop hadoop 168409  9월 17  2016 hive-1.2.1-site.xml
-rw-rw-r--.  1 hadoop hadoop 843090  3월 24  2013 mysql-connector-java-5.1.23-bin.jar
lrwxrwxrwx.  1 hadoop hadoop     10  5월 26 19:07 pig -> pig-0.16.0
drwxr-xr-x. 16 hadoop hadoop   4096  6월  2  2016 pig-0.16.0
lrwxrwxrwx.  1 hadoop hadoop     25  5월 26 19:08 spark -> spark-1.6.2-bin-hadoop2.6
drwxr-xr-x. 12 hadoop hadoop    210  6월 22  2016 spark-1.6.2-bin-hadoop2.6
~~~


#### 가상 머신 중단 및 VDI 이미지 백업하기


로그아웃 -> 다시 root 계정
~~~
[1]$ exit
~~~
디스크 용량 확인
~~~
[2]$ df
~~~
전원끄기 신호 보내기
~~~
[3]$ shutdown  -h  now
~~~

#### VDI 파일 백업하기

~~~
[hadoop@client ~]$ exit
logout
[root@client ~]# df
Filesystem     1K-blocks    Used Available Use% Mounted on
/dev/sda3        3770368 2867824    902544  77% /
devtmpfs          586868       0    586868   0% /dev
tmpfs             596464       0    596464   0% /dev/shm
tmpfs             596464    7784    588680   2% /run
tmpfs             596464       0    596464   0% /sys/fs/cgroup
/dev/sda1         201380   94904    106476  48% /boot
tmpfs             119296       0    119296   0% /run/user/0

[root@client ~]# shutdown  -h  now
~~~



#### 리눅스 환경 설정
=> 이제부터는 root가 아닌 hadoop 계정으로 작업합니다.

~~~
[1]$ cp  .bashrc  org_bashrc
[2]$ cp  bashrc2.txt  .bashrc
[3]$ source  .bashrc
[4]$ cat  .bashrc
[5]$ env
~~~

* /home/hadoop/.bashrc : hadoop 계정의 환경 설정 파일
* env : 환경 설정 조회 프로그램

~~~
[hadoop@client ~]$ cp  .bashrc  org_bashrc
[hadoop@client ~]$ cp  bashrc2.txt  .bashrc
[hadoop@client ~]$ source  .bashrc
[hadoop@client ~]$ cat .bashrc

# .bashrc

if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
~~~
~~~
# HADOOP Config Start
pathmunge () {
    case ":${PATH}:" in
        *:"$1":*)
            ;;
        *)
            if [ "$2" = "after" ] ; then
                PATH=$PATH:$1
            else
                PATH=$1:$PATH
            fi
    esac
}

export JAVA_HOME=/usr/local/java
export CLASSPATH=/usr/local/java/jre/lib/*
pathmunge /usr/local/java before
pathmunge /usr/local/java/bin before

export BASEHOME=/home/hadoop

export HADOOP_PREFIX=$BASEHOME/hadoop
export HADOOP_HOME=$BASEHOME/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
export HADOOP_CLASSPATH=$JAVA_HOME/lib/tools.jar

export PIG_HOME=$BASEHOME/pig
export PIG_CLASSPATH=$BASEHOME/hadoop/conf

export HIVE_HOME=$BASEHOME/hive
export HIVE_CONF_DIR=$BASEHOME/hive/conf
export HIVE_CLASS_PATH=$HIVE_CONF_DIR
export HADOOP_USER_CLASSPATH_FIRST=true

export SPARK_HOME=/home/hadoop/spark
export SPARK_CONF_DIR=$SPARK_HOME/conf

#pathmunge $BASEHOME/sqoop/bin
pathmunge $BASEHOME/pig/bin
pathmunge $BASEHOME/hive/bin
pathmunge $BASEHOME/hadoop/bin
~~~
HADOOP Config End

~~~
[hadoop@client ~]$ env
SPARK_HOME=/home/hadoop/spark
HOSTNAME=localhost.localdomain
PIG_HOME=/home/hadoop/pig
SHELL=/bin/bash
TERM=xterm
HIVE_CLASS_PATH=/home/hadoop/hive/conf
HADOOP_HOME=/home/hadoop/hadoop
HISTSIZE=1000
HADOOP_PREFIX=/home/hadoop/hadoop
SPARK_CONF_DIR=/home/hadoop/spark/conf
YARN_HOME=/home/hadoop/hadoop
USER=hadoop
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.tbz=01;31:*.tbz2=01;31:*.bz=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:
HADOOP_COMMON_LIB_NATIVE_DIR=/home/hadoop/hadoop/lib/native
MAIL=/var/spool/mail/hadoop
PATH=/home/hadoop/hadoop/bin:/home/hadoop/hive/bin:/home/hadoop/pig/bin:/usr/local/java/bin:/usr/local/java:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hadoop/bin
HADOOP_HDFS_HOME=/home/hadoop/hadoop
HIVE_HOME=/home/hadoop/hive
HADOOP_COMMON_HOME=/home/hadoop/hadoop
PWD=/home/hadoop
JAVA_HOME=/usr/local/java
HADOOP_CLASSPATH=/usr/local/java/lib/tools.jar
HADOOP_INSTALL=/home/hadoop/hadoop
PIG_CLASSPATH=/home/hadoop/hadoop/conf
HADOOP_CONF_DIR=/home/hadoop/hadoop/etc/hadoop
LANG=ko_KR.UTF-8
HADOOP_OPTS=-Djava.library.path=/home/hadoop/hadoop/lib/native
HISTCONTROL=ignoredups
SHLVL=1
HOME=/home/hadoop
HADOOP_MAPRED_HOME=/home/hadoop/hadoop
LOGNAME=hadoop
CLASSPATH=/usr/local/java/jre/lib/*
LESSOPEN=||/usr/bin/lesspipe.sh %s
BASEHOME=/home/hadoop
HADOOP_USER_CLASSPATH_FIRST=true
G_BROKEN_FILENAMES=1
HIVE_CONF_DIR=/home/hadoop/hive/conf
_=/bin/env
~~~

JDK 버전 확인
~~~
[6]$ javac -version
~~~

현재 실행되는 자바 프로세스 조회(jps만 나옴)
~~~
[7]$ jps
~~~
=> 현재 머신에서 실행되고 있는 자바 프로그램 조회하기
~~~
[hadoop@client ~]$ javac -version
javac 1.7.0_55

[hadoop@client ~]$ jps
1657 Jps
~~~

[Note] 1657 번호는 리눅스에서 실행되는 프로세스의
       Random ID 값(각자 다른 값을 가짐)


#### SSH 설정 : .ssh 디렉터리 생성하기

-> 클러스터의 수십,수백대의 머신에 접속하기 위해서

-> 가상머신을 복제하여 총 3대를 만든 후
   비밀번호 없이 SSH 접속이 가능하도록 설정

[NOTE] 처음 ssh 명령을 실행할 때 비밀번호를 물어보면 그냥 Enter를 3번.
~~~
[8]$ ssh localhost

[9]$ ls -al .ssh

[10]$ rm .ssh/known_hosts  
~~~

~~~
[hadoop@client ~]$ ssh localhost
Warning: Permanently added 'localhost' (RSA) to the list of known hosts.
hadoop@localhost's password:
Permission denied, please try again.
hadoop@localhost's password:
Permission denied, please try again.
hadoop@localhost's password:
Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).
~~~
~~~
[hadoop@client ~]$ ls -al  .ssh
합계 12
drwx------. 2 hadoop hadoop 4096 2017-05-06 16:50 .
drwx------. 3 hadoop hadoop 4096 2017-05-06 16:50 ..
-rw-r--r--. 1 hadoop hadoop  391 2017-05-06 16:50 known_hosts
[hadoop@client ~]$ rm .ssh/known_hosts
~~~
#### (4) 아파치 하둡 설정
###### 하둡 설정
하둡 설정 디렉터리 : /home/hadoop/hadoop/etc/hadoop

=> 다음에 나오는 7개의 파일을 편집하세요.
~~~
[2]$   hadoop-env.sh
       core-site.xml
       hdfs-site.xml
       mapred-site.xml
       yarn-site.xml
       log4j.properties
       slaves
~~~
1. 하둡 환경 설정 파일 : hadoop-env.sh
~~~
export JAVA_HOME=/usr/local/java

export HADOOP_PREFIX=${HADOOP_HOME}

export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native

export HADOOP_OPTS="${HADOOP_OPTS} -XX:-PrintWarnings -Djava.library.path=${HADOOP_PREFIX}/lib/native"
~~~

2. 하둡 코어 설정 : core-site.xml

- 네임노드 -> hdfs://namenode:8020/
~~~
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
 <property>
  <name>fs.defaultFS</name>
   <value>hdfs://namenode.hadoop.kr:8020/</value>
 </property>
 <property>
  <name>io.file.buffer.size</name>
   <value>131072</value>
 </property>
</configuration>
~~~

3. HDFS 설정 : hdfs-site.xml

- 복제인수는 1->3으로 변경해야 함
- HDFS 네임노드 웹UI 호스트명/포트           ->    namenode:50070
- HDFS 보조네임노드 웹UI 호스트명/포트       ->  secondnode:50090
- HDFS 데이터노드 웹UI 호스트명/포트         ->    datanode:50010 * 잘 안됨
~~~
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
 <property>
  <name>dfs.replication</name>
  <value>1</value>
 </property>
 <property>
  <name>dfs.namenode.http-address</name>
  <value>namenode.hadoop.kr:50070</value>
 </property>
 <property>
  <name>dfs.namenode.secondary.http-address</name>
  <value>secondnode.hadoop.kr:50090</value>
 </property>
 <property>
  <name>dfs.datanode.http-address</name>
  <value>0.0.0.0:50010</value>
 </property>
 <property>
  <name>dfs.namenode.name.dir</name>
   <value>file:///data/name1,file:///data/name2</value>
 </property>
 <property>
  <name>dfs.datanode.data.dir</name>
   <value>file:///data/data</value>
 </property>
 <property>
  <name>dfs.namenode.checkpoint.dir</name>
   <value>file:///data/namesecondary</value>
 </property>
</configuration>
~~~

4. MapReduce 2.0 설정 : mapred-site.xml
~~~
- mapreduce.jobhistory.address(IPC Port)  -> namenode:10020
- MapReduce 히스토리 서버 호스트명/포트   -> namenode:19888

<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
 <property>
  <name>mapreduce.framework.name</name>
   <value>yarn</value>
 </property>
 <property>
  <name>mapreduce.jobhistory.address</name>
   <value>namenode.hadoop.kr:10020</value>
 </property>
 <property>
  <name>mapreduce.jobhistory.webapp.address</name>
   <value>namenode.hadoop.kr:19888</value>
 </property>
</configuration>
~~~
5. YARN 설정 : yarn-site.xml

- YARN 리소스매니저 웹UI 호스트명/포트 -> secondnode:8088
- YARN 노드매니저 웹UI 호스트명/포트   -> namenode:8042 secondnode:8042 datanode(1-N):8042
~~~
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
 <property>
  <name>yarn.resourcemanager.hostname</name>
    <value>secondnode.hadoop.kr</value>
 </property>
 <property>
  <name>yarn.nodemanager.local-dirs</name>
    <value>/data/nm</value>
  </property>
 <property>
  <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
 </property>
 <property>
  <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>2</value>
 </property>
</configuration>
~~~

6. 하둡 로그 설정 : log4j.properties

- 로그 수준을 INFO에서 ERROR으로 변경해야 함

~~~
hadoop.root.logger=ERROR,console
~~~

7. 워커노드 설정 : slaves
~~~
datanode1.hadoop.kr
datanode2.hadoop.kr
~~~

**설정이 잘 안되면 다음 방법을 사용하세요!**

- 클러스터모드 하둡 설정 파일 다운로드
~~~
[0]$ cd
[1]$ wget http://www.db21.co.kr/hadoop/hdconf_2017.tgz
~~~
- 압축 풀기
~~~
[2]$ tar xvzf hdconf_2017.tgz
~~~
- 내려받은 설정 파일 확인
~~~
[3]$ ll etc_hadoop_2017
~~~
- 설정 파일을 하둡 설정 디렉터리에 덮어쓰기
~~~
[4]$ cp etc_hadoop_2017/*  hadoop/etc/hadoop/

[5]$ echo 'datanode1.hadoop.kr' > hadoop/etc/hadoop/slaves
     echo 'datanode2.hadoop.kr' >> hadoop/etc/hadoop/slaves

[6]$ cat hadoop/etc/hadoop/slaves
~~~

~~~
[hadoop@client ~]$ wget http://www.db21.co.kr/hadoop/hdconf_2017.tgz
--2017-06-01 14:27:00--  http://www.db21.co.kr/hadoop/hdconf_2017.tgz
Resolving www.db21.co.kr (www.db21.co.kr)... 121.124.124.244
Connecting to www.db21.co.kr (www.db21.co.kr)|121.124.124.244|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4465 (4.4K) [application/x-tar]
Saving to: ‘hdconf_2017.tgz’

100%[======================================>] 4,465       --.-K/s   in 0.006s

2017-06-01 14:27:00 (788 KB/s) - ‘hdconf_2017.tgz’ saved [4465/4465]

[hadoop@client ~]$  tar xvzf hdconf_2017.tgz
etc_hadoop_2017/
etc_hadoop_2017/hadoop-env.sh
etc_hadoop_2017/yarn-site.xml
etc_hadoop_2017/hdfs-site.xml
etc_hadoop_2017/core-site.xml
etc_hadoop_2017/log4j.properties
etc_hadoop_2017/mapred-site.xml

[hadoop@client ~]$ ll etc_hadoop_2017
합계 32
-rw-r--r--. 1 hadoop hadoop   321  5월 25 16:56 core-site.xml
-rw-r--r--. 1 hadoop hadoop  2737  5월 25 16:57 hadoop-env.sh
-rw-r--r--. 1 hadoop hadoop   862  5월 25 16:56 hdfs-site.xml
-rw-r--r--. 1 hadoop hadoop 11292  5월 25 16:57 log4j.properties
-rw-rw-r--. 1 hadoop hadoop   453  6월  1 14:25 mapred-site.xml
-rw-r--r--. 1 hadoop hadoop   545  5월 25 16:56 yarn-site.xml

[hadoop@client ~]$ cp etc_hadoop_2017/*  hadoop/etc/hadoop/

[hadoop@client ~]$ echo 'datanode1.hadoop.kr' > hadoop/etc/hadoop/slaves
[hadoop@client ~]$ echo 'datanode2.hadoop.kr' >> hadoop/etc/hadoop/slaves

[hadoop@client ~]$ cat hadoop/etc/hadoop/slaves                    
datanode1.hadoop.kr
datanode2.hadoop.kr
~~~

#### (5) 가상 머신 복제하기

로그아웃 => hadoop 계정에서 다시 root 계정으로
~~~
[1]$ exit
~~~
root 계정 확인
~~~
[2]$ whoami
~~~
가상머신 종료
~~~
[3]$ shutdown  -h  now
~~~

=> Virtual Box에서 작업합니다.

클러스터 1번 hd1-client 머신을 선택한 후

[복제]를 통해서 가상 머신 2대 만들기

**클러스터 2번**

새 머신 이름 : hd2-namenode
[v] 모든 네트워크 카드의 MAC 주소 초기화

복제 방식 : 완전한 복제

**클러스터 3번**

새 머신 이름 : hd3-secondnode
[v] 모든 네트워크 카드의 MAC 주소 초기화

복제 방식 : 완전한 복제

[NOTE] 복제된 2번,3번 머신의 네트워크 설정은 추가로 할 필요가 없음
       - MAC 주소 신경쓰지 마세요

#### (6) 가상 머신 고정 IP 설정(콘솔에서)

=> 가상 머신 3대를 모두 시작( 전원 On )

**클러스터 1번 머신 [ hd1-client ]**

=> 가상 머신 콘솔에서 직접
~~~
[1]$ ip  a
[2]$ ifconfig  enp0s8  192.168.56.21
[3]$ ip  a
~~~

**클러스터 2번 머신 [ hd2-namenode ]**

=> 가상 머신 콘솔에서 직접
~~~
[1]$ ip  a
[2]$ ifconfig  enp0s8  192.168.56.31
[3]$ ip  a
~~~
** 클러스터 3번 머신 [ hd3-secondnode ]**

=> 가상 머신 콘솔에서 직접
~~~
[1]$ ip  a
[2]$ ifconfig  enp0s8  192.168.56.41
[3]$ ip  a
~~~
#### (7) 클러스터 3대 머신에 개별 네트워크 설정

=> putty.exe로 클러스터 1번 머신 [ hd1-client ]에 ssh 접속

- IP : 192.168.56.21

=> 자신의 호스트 이름 설정 및 부팅시 고정 IP 사용하기 설정

**클러스터 1번 머신 [ hd1-client ]**

~~~
[1]$ hostname
[2]$ echo 'ifconfig  enp0s8  192.168.56.21' >> /etc/rc.d/rc.local
[3]$ chmod  +x  /etc/rc.d/rc.local
~~~

[NOTE] /etc/rc.d/rc.local 파일에 부팅시 자동으로 실행할 명령어를 추가



**클러스터 2번 머신 [ hd2-namenode ]**

=> client에서 2번 머신으로 접속
~~~
[4]$ ssh namenode
~~~
호스트명을 변경하고 확인
~~~
[5]$ echo 'namenode.hadoop.kr' > /etc/hostname
[6]$ hostname namenode.hadoop.kr
[7]$ hostname
~~~
부팅시 고정 IP 사용하기 설정
~~~
[8]$ echo 'ifconfig  enp0s8  192.168.56.31' >> /etc/rc.d/rc.local
[9]$ chmod  +x  /etc/rc.d/rc.local
[10]$ w
[11]$ exit
~~~
=> 다시 들어가서 Prompt가 [root@namenode ~]# 으로 변경된 것을 확인
~~~
[12]$ ssh namenode
[13]$ exit
~~~
=> 2번 머신에서 로그아웃하여 client로 돌아감

~~~
[root@client ~]# ssh namenode
Warning: Permanently added 'namenode,192.168.56.31' (ECDSA) to the list of known hosts.
root@namenode's password:
Last login: Thu Jun  1 16:46:31 2017

[root@client ~]# echo 'namenode.hadoop.kr' > /etc/hostname
[root@client ~]# hostname namenode.hadoop.kr
[root@client ~]# hostname
namenode.hadoop.kr

[root@client ~]# echo 'ifconfig  enp0s8  192.168.56.31' >> /etc/rc.d/rc.local
[root@client ~]# chmod  +x  /etc/rc.d/rc.local

[root@client ~]# w
 17:00:25 up 15 min,  2 users,  load average: 0.00, 0.01, 0.04
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:46   13:37   0.04s  0.04s -bash
root     pts/0    client.hadoop.kr 16:57    1.00s  0.02s  0.01s w
[root@client ~]# exit
logout
Connection to namenode closed.
[root@client ~]#


[root@client ~]# ssh namenode
root@namenode's password:
Last login: Thu Jun  1 16:57:28 2017 from client.hadoop.kr
[root@namenode ~]# exit
logout
Connection to namenode closed.
~~~

? 여기는 어디인가 ?
~~~
[root@client ~]# w
 17:08:16 up 13 min,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:55   12:56   0.02s  0.02s -bash
root     pts/0    192.168.56.1     16:55    8.00s  0.04s  0.03s w
~~~

**클러스터 3번 머신 [ hd3-secondnode ]**

=> client에서 3번 머신으로 접속

~~~
[14]$ ssh secondnode
~~~

호스트명을 변경하고 확인
~~~
[15]$ echo 'secondnode.hadoop.kr' > /etc/hostname
[16]$ hostname secondnode.hadoop.kr
[17]$ hostname
~~~
부팅시 고정 IP 사용하기 설정
~~~
[18]$ echo 'ifconfig  enp0s8  192.168.56.41' >> /etc/rc.d/rc.local
[19]$ chmod  +x  /etc/rc.d/rc.local
[20]$ w
[21]$ exit
~~~
=> 다시 들어가서 Prompt가 [root@secondnode ~]# 으로 변경된 것을 확인
~~~
[22]$ ssh secondnode
[23]$ exit
~~~
=> 3번 머신에서 로그아웃하여 client로 돌아감


~~~
[root@client ~]# ssh secondnode
Warning: Permanently added 'secondnode,192.168.56.41' (ECDSA) to the list of known hosts.
root@secondnode's password:
Last login: Thu Jun  1 16:47:23 2017

[root@client ~]# echo 'secondnode.hadoop.kr' > /etc/hostname
[root@client ~]# hostname secondnode.hadoop.kr
[root@client ~]# hostname
secondnode.hadoop.kr

[root@client ~]# echo 'ifconfig  enp0s8  192.168.56.41' >> /etc/rc.d/rc.local
[root@client ~]# chmod  +x  /etc/rc.d/rc.local

[root@client ~]# w
 17:10:32 up 23 min,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:47   22:56   0.04s  0.04s -bash
root     pts/0    client.hadoop.kr 17:08    0.00s  0.02s  0.01s w
[root@client ~]# exit
logout
Connection to secondnode closed.


[root@client ~]# ssh secondnode
root@secondnode's password:
Last login: Thu Jun  1 17:08:50 2017 from client.hadoop.kr

[root@secondnode ~]# exit
logout
Connection to secondnode closed.
[root@client ~]#
~~~
####(8) 비밀번호 없이 클러스터의 다른 머신에 접속하기 설정


개인키와 공개키를 만들고 접속대상 머신의 키박스에 공개키를 등록

=> 개인키(열쇠) - 공개키(자물쇠) : id_dsa - id_dsa.pub

=> 키박스에 다수의 공개키를 등록할 수 있음 : authorized_keys


=---> 이제부터는 hadoop 계정으로 하는 작업입니다.
~~~
[1]$ su  -  hadoop
[2]$ whoami
[3]$ hostname
~~~

~~~
[root@client ~]# su - hadoop
마지막 로그인: 목  6월  1 16:18:07 KST 2017 일시 pts/0

[hadoop@client ~]$ whoami
hadoop
[hadoop@client ~]$ hostname
client.hadoop.kr
[hadoop@client ~]$
~~~

**클러스터 1번 머신 [ hd1-client ]**

개인키와 공개키를 생성
~~~
[1]$ ssh-keygen   -t    dsa    -P    ''    -f    ~/.ssh/id_dsa
~~~

공개키를 키박스에 추가
~~~
[2]$ cat   ~/.ssh/id_dsa.pub   >>  ~/.ssh/authorized_keys
~~~

접근권한 변경 : 다른 사람이 보면 안되므로
~~~
[3]$ chmod   400   ~/.ssh/authorized_keys
~~~
=> 읽기만 할 수 있도록 접근 권한을 변경해야 함

- 확인하기
~~~
[4]$ ls -al  .ssh
[5]$ ssh localhost
     w
     exit
~~~

~~~
[hadoop@client ~]$ ssh-keygen   -t    dsa    -P    ''    -f    ~/.ssh/id_dsa
Generating public/private dsa key pair.
Your identification has been saved in /home/hadoop/.ssh/id_dsa.
Your public key has been saved in /home/hadoop/.ssh/id_dsa.pub.
The key fingerprint is:
dd:ec:0f:38:06:ef:ba:46:8f:1c:2e:5e:a6:70:d5:fd hadoop@client.hadoop.kr
The key's randomart image is:
+--[ DSA 1024]----+
|                 |
|                 |
|                 |
|         o +     |
|        S o +    |
|       .oo o .   |
|    . .+o+= o E  |
|     o.+=o.. o   |
|     .oooo.   .  |
+-----------------+
[hadoop@client ~]$ cat   ~/.ssh/id_dsa.pub   >>  ~/.ssh/authorized_keys
[hadoop@client ~]$ chmod   400   ~/.ssh/authorized_keys
[hadoop@client ~]$ ls -al  .ssh
합계 16
drwx------. 2 hadoop hadoop   61  6월  1 17:30 .
drwx------. 8 hadoop hadoop 4096  6월  1 16:27 ..
-r--------. 1 hadoop hadoop  613  6월  1 17:30 authorized_keys
-rw-------. 1 hadoop hadoop  672  6월  1 17:30 id_dsa
-rw-r--r--. 1 hadoop hadoop  613  6월  1 17:30 id_dsa.pub
[hadoop@client ~]$ ssh localhost
Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
Last login: Thu Jun  1 17:29:40 2017
[hadoop@client ~]$ w
 17:31:28 up 36 min,  3 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:55   36:08   0.02s  0.02s -bash
root     pts/0    192.168.56.1     16:55    0.00s  0.05s  0.01s ssh localhos
hadoop   pts/1    localhost        17:31    0.00s  0.01s  0.01s w
[hadoop@client ~]$ exit
logout
Connection to localhost closed.
~~~

1번 머신(client)의 공개키를 2번,3번 머신에 전송

~~~
[6]$ scp ~/.ssh/id_dsa.pub  hadoop@namenode://home/hadoop/.ssh/client.pub
[7]$ scp ~/.ssh/id_dsa.pub  hadoop@secondnode://home/hadoop/.ssh/client.pub
~~~

**클러스터 2번 머신 [ hd2-namenode ]**
~~~
[1]$ ssh namenode
~~~
개인키와 공개키를 생성
~~~
[2]$ ssh-keygen   -t    dsa    -P    ''    -f    ~/.ssh/id_dsa
~~~
공개키 이름을 식별하기 쉽게 변경
~~~
[3]$ mv ~/.ssh/id_dsa.pub ~/.ssh/namenode.pub
~~~

공개키를 3번 머신에만 전송
~~~
[4-1]$ scp ~/.ssh/namenode.pub  hadoop@secondnode://home/hadoop/.ssh/namenode.pub
~~~
- 공개키를 다른 워커에도 전송
~~~
[4-2]$ scp ~/.ssh/namenode.pub  hadoop@datanode1://home/hadoop/.ssh/namenode.pub
[4-3]$ scp ~/.ssh/namenode.pub  hadoop@datanode2://home/hadoop/.ssh/namenode.pub
[5]$ exit
~~~

~~~
[hadoop@client ~]$ ssh namenode
hadoop@namenode's password:
Last login: Thu Jun  1 17:35:20 2017

[hadoop@namenode ~]$ ssh-keygen   -t    dsa    -P    ''    -f    ~/.ssh/id_dsa
Generating public/private dsa key pair.
Your identification has been saved in /home/hadoop/.ssh/id_dsa.
Your public key has been saved in /home/hadoop/.ssh/id_dsa.pub.
The key fingerprint is:
ec:f7:fe:b4:e1:ab:a4:3d:a8:72:50:db:8c:60:4a:14 hadoop@namenode.hadoop.kr
The key's randomart image is:
+--[ DSA 1024]----+
|    E.           |
|    .            |
|   .             |
|    . o..        |
|   . o oS=       |
|    . ..o o      |
|       .. ... o  |
|      . ...=.o o |
|       o....=+=. |
+-----------------+

[hadoop@namenode ~]$ mv ~/.ssh/id_dsa.pub ~/.ssh/namenode.pub

[hadoop@namenode ~]$ scp ~/.ssh/namenode.pub  hadoop@secondnode://home/hadoop/.ssh/namenode.pub
Warning: Permanently added 'secondnode,192.168.56.41' (ECDSA) to the list of known hosts.
hadoop@secondnode's password:
namenode.pub                               100%  615     0.6KB/s   00:00

[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$
~~~

**클러스터 3번 머신 [ hd3-secondnode ]**
~~~
[1]$ ssh secondnode
~~~

개인키와 공개키를 생성
~~~
[2]$ ssh-keygen   -t    dsa    -P    ''    -f    ~/.ssh/id_dsa
~~~

공개키 이름을 식별하기 쉽게 변경
~~~
[3]$ mv ~/.ssh/id_dsa.pub ~/.ssh/secondnode.pub
~~~

공개키를 2번 머신에만 전송
~~~
[4]$ scp ~/.ssh/secondnode.pub  hadoop@namenode://home/hadoop/.ssh/secondnode.pub
[5]$ exit
~~~
~~~
[hadoop@client ~]$ ssh secondnode
Warning: Permanently added 'secondnode,192.168.56.41' (ECDSA) to the list of known hosts.
hadoop@secondnode's password:
Last failed login: Thu Jun  1 16:21:15 KST 2017 from localhost on ssh:notty
There were 2 failed login attempts since the last successful login.
Last login: Thu Jun  1 16:18:07 2017

[hadoop@secondnode ~]$ ssh-keygen   -t    dsa    -P    ''    -f    ~/.ssh/id_dsa
Generating public/private dsa key pair.
Your identification has been saved in /home/hadoop/.ssh/id_dsa.
Your public key has been saved in /home/hadoop/.ssh/id_dsa.pub.
The key fingerprint is:
46:af:4c:6f:94:ab:17:bb:4b:67:83:42:83:56:f1:3e hadoop@secondnode.hadoop.kr
The key's randomart image is:
+--[ DSA 1024]----+
|        .        |
|         o       |
|        o .      |
|       + o .     |
|      o S E      |
|     . = =.+     |
|        + *o+    |
|         =oo .   |
|        ..oo     |
+-----------------+

[hadoop@secondnode ~]$ mv ~/.ssh/id_dsa.pub ~/.ssh/secondnode.pub

[hadoop@secondnode ~]$ scp ~/.ssh/secondnode.pub  hadoop@namenode://home/hadoop/.ssh/secondnode.pub
Warning: Permanently added 'namenode,192.168.56.31' (ECDSA) to the list of known hosts.
hadoop@namenode's password:
secondnode.pub                             100%  617     0.6KB/s   00:00

[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@client ~]$
~~~


**클러스터 2번 머신 [ hd2-namenode ]**
~~~

[1]$ ssh namenode
[2]$ ll  .ssh
~~~

공개키(client, namenode, secondnode)들을 키박스에 추가
~~~
[3]$ cat   ~/.ssh/client.pub   >>  ~/.ssh/authorized_keys
     cat   ~/.ssh/namenode.pub   >>  ~/.ssh/authorized_keys
     cat   ~/.ssh/secondnode.pub   >>  ~/.ssh/authorized_keys
~~~

접근권한 변경 : 다른 사람이 보면 안되므로
~~~
[4]$ chmod   400   ~/.ssh/authorized_keys
[5]$ exit
~~~

~~~
[hadoop@client ~]$ ssh namenode
hadoop@namenode's password:
Last login: Thu Jun  1 17:37:54 2017 from client.hadoop.kr

[hadoop@namenode ~]$ ll .ssh
합계 20
-rw-r--r--. 1 hadoop hadoop 613  6월  1 17:35 client.pub
-rw-------. 1 hadoop hadoop 672  6월  1 17:38 id_dsa
-rw-r--r--. 1 hadoop hadoop 186  6월  1 17:41 known_hosts
-rw-r--r--. 1 hadoop hadoop 615  6월  1 17:38 namenode.pub
-rw-r--r--. 1 hadoop hadoop 617  6월  1 17:45 secondnode.pub

[hadoop@namenode ~]$ cat   ~/.ssh/client.pub   >>  ~/.ssh/authorized_keys
[hadoop@namenode ~]$ cat   ~/.ssh/namenode.pub   >>  ~/.ssh/authorized_keys
[hadoop@namenode ~]$ cat   ~/.ssh/secondnode.pub   >>  ~/.ssh/authorized_keys

[hadoop@namenode ~]$ chmod   400   ~/.ssh/authorized_keys

[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$
~~~

**클러스터 3번 머신 [ hd3-secondnode ]**

~~~
[1]$ ssh secondnode
[2]$ ll  .ssh
~~~

공개키(client, namenode, secondnode)들을 키박스에 추가
~~~
[3]$ cat   ~/.ssh/client.pub   >>  ~/.ssh/authorized_keys
     cat   ~/.ssh/namenode.pub   >>  ~/.ssh/authorized_keys
     cat   ~/.ssh/secondnode.pub   >>  ~/.ssh/authorized_keys
~~~

접근권한 변경 : 다른 사람이 보면 안되므로
~~~
[4]$ chmod   400   ~/.ssh/authorized_keys
[5]$ exit
~~~

~~~
[hadoop@client ~]$  ssh secondnode
hadoop@secondnode's password:
Last login: Thu Jun  1 17:50:50 2017 from client.hadoop.kr

[hadoop@secondnode ~]$ ll  .ssh
합계 20
-rw-r--r--. 1 hadoop hadoop 613  6월  1 17:52 client.pub
-rw-------. 1 hadoop hadoop 672  6월  1 17:44 id_dsa
-rw-r--r--. 1 hadoop hadoop 184  6월  1 17:45 known_hosts
-rw-r--r--. 1 hadoop hadoop 615  6월  1 17:41 namenode.pub
-rw-r--r--. 1 hadoop hadoop 617  6월  1 17:44 secondnode.pub

[hadoop@secondnode ~]$  cat   ~/.ssh/client.pub   >>  ~/.ssh/authorized_keys
[hadoop@secondnode ~]$  cat   ~/.ssh/namenode.pub   >>  ~/.ssh/authorized_keys
[hadoop@secondnode ~]$  cat   ~/.ssh/secondnode.pub   >>  ~/.ssh/authorized_keys

[hadoop@secondnode ~]$ chmod   400   ~/.ssh/authorized_keys

[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@client ~]$
~~~
=---> 비밀번호 없이 다른 머신에 접속하기 : 확인 작업( 중요 )

**클러스터 1번 머신 [ hd1-client ]**

- 2번과 3번 머신으로 ssh 접속 테스트
~~~
[1]$ ssh namenode
     w
     exit
[2]$ ssh secondnode
     w
     exit
~~~

~~~
[hadoop@client ~]$ ssh namenode
Last login: Thu Jun  1 17:48:33 2017 from client.hadoop.kr
[hadoop@namenode ~]$ w
 17:55:54 up  1:10,  2 users,  load average: 0.00, 0.04, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:46   20:34   0.07s  0.01s -bash
hadoop   pts/0    client.hadoop.kr 17:55    2.00s  0.01s  0.01s w
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.

[hadoop@client ~]$ ssh secondnode
Last login: Thu Jun  1 17:52:38 2017 from client.hadoop.kr
[hadoop@secondnode ~]$ w
 17:56:07 up  1:09,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:47    1:08m  0.04s  0.04s -bash
hadoop   pts/0    client.hadoop.kr 17:56    1.00s  0.01s  0.01s w
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@client ~]$
~~~

**클러스터 2번 머신 [ hd2-namenode ]**

=> 먼저 2번 머신으로 ssh 접속
~~~
[1]$ ssh namenode
~~~
- 자신과 3번 머신으로 ssh 접속 테스트
~~~
[2]$ ssh localhost
     w
     exit
[3]$ ssh namenode
     w
     exit
[4]$ ssh secondnode
     w
     exit
~~~
=> 2번 머신에서 로그아웃해서 다시 client 머신으로
~~~
[5]  exit
[6]  w
~~~

~~~
[hadoop@client ~]$ ssh namenode
Last login: Thu Jun  1 17:55:50 2017 from client.hadoop.kr
[hadoop@namenode ~]$ ssh localhost
Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
Last login: Thu Jun  1 17:59:22 2017 from client.hadoop.kr
[hadoop@namenode ~]$ w
 17:59:35 up  1:14,  3 users,  load average: 0.01, 0.03, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:46   24:15   0.07s  0.01s -bash
hadoop   pts/0    client.hadoop.kr 17:59    7.00s  0.02s  0.01s ssh localhos
hadoop   pts/1    localhost        17:59    6.00s  0.01s  0.01s w
[hadoop@namenode ~]$ exit
logout
Connection to localhost closed.
[hadoop@namenode ~]$ ssh namenode
Warning: Permanently added 'namenode,192.168.56.31' (ECDSA) to the list of known hosts.
Last login: Thu Jun  1 17:59:29 2017 from localhost
[hadoop@namenode ~]$ w
 17:59:57 up  1:14,  3 users,  load average: 0.01, 0.03, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:46   24:37   0.07s  0.01s -bash
hadoop   pts/0    client.hadoop.kr 17:59    5.00s  0.03s  0.02s ssh namenode
hadoop   pts/1    namenode.hadoop. 17:59    2.00s  0.01s  0.01s w
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@namenode ~]$ ssh secondnode
Last login: Thu Jun  1 17:56:06 2017 from client.hadoop.kr
[hadoop@secondnode ~]$ w
 18:00:14 up  1:13,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:47    1:12m  0.04s  0.04s -bash
hadoop   pts/0    namenode.hadoop. 18:00    2.00s  0.01s  0.01s w
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$ w
 18:00:21 up  1:05,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:55    1:05m  0.02s  0.02s -bash
root     pts/0    192.168.56.1     16:55    5.00s  0.06s  0.00s w
[hadoop@client ~]$
~~~

**클러스터 3번 머신 [ hd3-secondnode ]**

=> 먼저 3번 머신으로 ssh 접속
~~~
[1]$ ssh secondnode
~~~
자신과 2번 머신으로 ssh 접속 테스트
~~~
[2]$ ssh localhost
     w
     exit
[3]$ ssh namenode
     w
     exit
[4]$ ssh secondnode
     w
     exit
~~~
=> 3번 머신에서 로그아웃해서 다시 client 머신으로
~~~
[5]  exit
[6]  w
~~~

~~~
[hadoop@namenode ~]$ ssh secondnode
Last login: Thu Jun  1 17:56:06 2017 from client.hadoop.kr
[hadoop@secondnode ~]$ w
 18:00:14 up  1:13,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:47    1:12m  0.04s  0.04s -bash
hadoop   pts/0    namenode.hadoop. 18:00    2.00s  0.01s  0.01s w
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$ w
 18:00:21 up  1:05,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:55    1:05m  0.02s  0.02s -bash
root     pts/0    192.168.56.1     16:55    5.00s  0.06s  0.00s w
[hadoop@client ~]$
[hadoop@client ~]$
[hadoop@client ~]$
[hadoop@client ~]$ clear
[hadoop@client ~]$ ssh secondnode
Last login: Thu Jun  1 18:00:12 2017 from namenode.hadoop.kr
[hadoop@secondnode ~]$ ssh localhost
Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
Last login: Thu Jun  1 18:02:16 2017 from client.hadoop.kr
[hadoop@secondnode ~]$ w
 18:02:29 up  1:15,  3 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:47    1:14m  0.04s  0.04s -bash
hadoop   pts/0    client.hadoop.kr 18:02    5.00s  0.02s  0.02s ssh localhos
hadoop   pts/1    localhost        18:02    3.00s  0.01s  0.01s w
[hadoop@secondnode ~]$ exit
logout
Connection to localhost closed.
[hadoop@secondnode ~]$ ssh namenode
Last login: Thu Jun  1 17:59:55 2017 from namenode.hadoop.kr
[hadoop@namenode ~]$ w
 18:02:40 up  1:17,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:46   27:20   0.07s  0.01s -bash
hadoop   pts/0    secondnode.hadoo 18:02    0.00s  0.01s  0.01s w
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@secondnode ~]$ ssh secondnode
Warning: Permanently added 'secondnode,192.168.56.41' (ECDSA) to the list of known hosts.
Last login: Thu Jun  1 18:02:26 2017 from localhost
[hadoop@secondnode ~]$ w
 18:02:49 up  1:16,  3 users,  load average: 0.56, 0.13, 0.08
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:47    1:15m  0.04s  0.04s -bash
hadoop   pts/0    client.hadoop.kr 18:02    1.00s  0.02s  0.01s ssh secondno
hadoop   pts/1    secondnode.hadoo 18:02    1.00s  0.01s  0.01s w
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@client ~]$ w
 18:02:52 up  1:08,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      16:55    1:07m  0.02s  0.02s -bash
root     pts/0    192.168.56.1     16:55    4.00s  0.06s  0.00s w
[hadoop@client ~]$
~~~



#### (9) HDFS 포맷하기

##### HDFS 포맷

[NOTE] HDFS 포맷은 네임노드에서 해야 함

- 1번 머신에서 2번 머신으로 SSH 접속
~~~
[1]$ ssh namenode
~~~
- 호스트명 반드시 확인하세요!
~~~
[2]$ hostname
~~~

처음 한 번만 HDFS 포맷하기
~~~
[3]$ hadoop/bin/hdfs  namenode  -format
~~~

파일시스템 생성 확인
~~~
[4]$ ls /data
     ls -R /data
~~~

로그아웃하여 다시 1먼 머신으로
~~~
[5]$ exit
~~~

~~~
[hadoop@client ~]$ ssh namenode
Last login: Thu Jun  1 18:02:38 2017 from secondnode.hadoop.kr
[hadoop@namenode ~]$ hostname
namenode.hadoop.kr
[hadoop@namenode ~]$ hadoop/bin/hdfs  namenode  -format
17/06/07 14:34:02 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = namenode.hadoop.kr/192.168.56.31
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 2.6.4
...
...

17/06/07 14:34:05 INFO common.Storage: Storage directory /data/name1 has been successfully formatted.
17/06/07 14:34:05 INFO common.Storage: Storage directory /data/name2 has been successfully formatted.
17/06/07 14:34:05 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
17/06/07 14:34:05 INFO util.ExitUtil: Exiting with status 0
17/06/07 14:34:05 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at namenode.hadoop.kr/192.168.56.31
************************************************************/

[hadoop@namenode ~]$ ls /data
name1  name2

[hadoop@namenode ~]$ ls -R /data
/data:
name1  name2

/data/name1:
current

/data/name1/current:
VERSION                      fsimage_0000000000000000000.md5
fsimage_0000000000000000000  seen_txid

/data/name2:
current

/data/name2/current:
VERSION                      fsimage_0000000000000000000.md5
fsimage_0000000000000000000  seen_txid
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$
~~~
### (10) 하둡 서비스 시작하기
- HDFS, YARN, MapReduce
------------------------------------------------------------
------------------------------------------------------------


#### 1. HDFS 서비스 시작

~~~
[NOTE] HDFS 서비스 시작과 종료는 네임노드에서 해야 함

-> 1번 머신에서 2번 머신으로 SSH 접속
[1]$ ssh namenode

- 호스트명 반드시 확인하세요!
[2]$ hostname

- 자바 프로세스 조회
[3]$ jps

- HDFS 서비스 시작
[4]$ hadoop/sbin/start-dfs.sh

- 2번 머신에서 HDFS 자바 데몬 확인
[5]$ jps

-> 2번 머신에서 3번 머신으로 SSH 접속
[6]$ ssh secondnode

-> 3번 머신에서 HDFS 자바 데몬 확인
[7]$ jps

<- 로그아웃하여 다시 2번 머신으로
[8]$ exit

<- 로그아웃하여 다시 1번 머신으로
[9]$ exit
~~~

~~~
[hadoop@client ~]$ ssh namenode
Last login: Wed Jun  7 14:33:51 2017 from client.hadoop.kr
[hadoop@namenode ~]$ hostname
namenode.hadoop.kr
[hadoop@namenode ~]$ jps
1227 Jps
[hadoop@namenode ~]$ hadoop/sbin/start-dfs.sh
Starting namenodes on [namenode.hadoop.kr]
namenode.hadoop.kr: starting namenode, logging to /home/hadoop/hadoop/logs/hadoop-hadoop-namenode-namenode.hadoop.kr.out
datanode1.hadoop.kr: starting datanode, logging to /home/hadoop/hadoop/logs/hadoop-hadoop-datanode-namenode.hadoop.kr.out
datanode2.hadoop.kr: starting datanode, logging to /home/hadoop/hadoop/logs/hadoop-hadoop-datanode-secondnode.hadoop.kr.out
Starting secondary namenodes [secondnode.hadoop.kr]
secondnode.hadoop.kr: starting secondarynamenode, logging to /home/hadoop/hadoop/logs/hadoop-hadoop-secondarynamenode-secondnode.hadoop.kr.out
[hadoop@namenode ~]$ jps
2017 NameNode
2326 Jps
2139 DataNode
[hadoop@namenode ~]$ ssh secondnode
Last login: Thu Jun  1 18:02:47 2017 from secondnode.hadoop.kr
[hadoop@secondnode ~]$ jps
1294 Jps
1147 DataNode
1235 SecondaryNameNode
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$
~~~
--------------------------------------------------------

#### 2. YARN 서비스 시작

[NOTE] YARN 서비스 시작과 종료는 리소스매니저 노드에서 해야 함
~~~
-> 1번 머신에서 3번 머신으로 SSH 접속
[1]$ ssh secondnode

- 호스트명 반드시 확인하세요!
[2]$ hostname

- 자바 프로세스 조회
[3]$ jps

- YARN 서비스 시작
[4]$ hadoop/sbin/start-yarn.sh

- 3번 머신에서 YARN 자바 데몬 확인
[5]$ jps

-> 3번 머신에서 2번 머신으로 SSH 접속
[6]$ ssh namenode

-> 2번 머신에서 HDFS 자바 데몬 확인
[7]$ jps

<- 로그아웃하여 다시 3번 머신으로
[8]$ exit

<- 로그아웃하여 다시 1번 머신으로
[9]$ exit
~~~


~~~
[hadoop@client ~]$  ssh secondnode
Last login: Wed Jun  7 14:49:54 2017 from namenode.hadoop.kr
[hadoop@secondnode ~]$ hostname
secondnode.hadoop.kr
[hadoop@secondnode ~]$ jps
1424 Jps
1147 DataNode
1235 SecondaryNameNode
[hadoop@secondnode ~]$ hadoop/sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /home/hadoop/hadoop/logs/yarn-hadoop-resourcemanager-secondnode.hadoop.kr.out
datanode1.hadoop.kr: starting nodemanager, logging to /home/hadoop/hadoop/logs/yarn-hadoop-nodemanager-namenode.hadoop.kr.out
datanode2.hadoop.kr: starting nodemanager, logging to /home/hadoop/hadoop/logs/yarn-hadoop-nodemanager-secondnode.hadoop.kr.out

[hadoop@secondnode ~]$ jps
1700 Jps
1147 DataNode
1560 NodeManager
1235 SecondaryNameNode
1466 ResourceManager
[hadoop@secondnode ~]$ ssh namenode
Last login: Wed Jun  7 14:42:54 2017 from client.hadoop.kr
[hadoop@namenode ~]$ jps
2017 NameNode
2557 Jps
2427 NodeManager
2139 DataNode
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@client ~]$
~~~
#### 3. MapReduce 서비스 시작
~~~
[NOTE] 잡히스토리 서버 시작과 종료는 네임노드

-> 1번 머신에서 2번 머신으로 SSH 접속
[1]$ ssh namenode

- 호스트명 반드시 확인하세요!
[2]$ hostname

- 자바 프로세스 조회
[3]$ jps

- 잡히스토리 서버 시작
[4]$ hadoop/sbin/mr-jobhistory-daemon.sh start historyserver

- 2번 머신에서 잡히스토리 서버 데몬 확인
[5]$ jps

<- 로그아웃하여 다시 1번 머신으로
[6]$ exit
~~~
~~~
[hadoop@client ~]$ ssh namenode
Last login: Wed Jun  7 15:05:59 2017 from secondnode.hadoop.kr
[hadoop@namenode ~]$ hostname
namenode.hadoop.kr
[hadoop@namenode ~]$ jps
2017 NameNode
2427 NodeManager
2139 DataNode
2616 Jps
[hadoop@namenode ~]$ hadoop/sbin/mr-jobhistory-daemon.sh start historyserver
starting historyserver, logging to /home/hadoop/hadoop/logs/mapred-hadoop-historyserver-namenode.hadoop.kr.out


[hadoop@namenode ~]$ jps
2017 NameNode
2645 JobHistoryServer
2709 Jps
2427 NodeManager
2139 DataNode
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$
~~~
###
(11) HDFS 테스트하기

[NOTE] 하둡 HDFS, MapReduce 테스트는 클라이언트 노드에서
~~~
[1]$ hostname

[2]$  hadoop  fs  -lsr /
      hdfs dfs -chmod 777 /tmp
      hdfs dfs -mkdir /user
      hdfs dfs -mkdir /user/hadoop
      echo 'test' > test.txt
      hdfs dfs -put test.txt /user/hadoop/hadooptest.txt
      hdfs dfs -ls -R /user
      hdfs dfs -ls
~~~
~~~
[hadoop@client ~]$ hostname
client.hadoop.kr
[hadoop@client ~]$   hadoop  fs  -lsr /
lsr: DEPRECATED: Please use 'ls -R' instead.
drwxrwx---   - hadoop supergroup          0 2017-06-07 15:13 /tmp
drwxrwx---   - hadoop supergroup          0 2017-06-07 15:13 /tmp/hadoop-yarn
drwxrwx---   - hadoop supergroup          0 2017-06-07 15:13 /tmp/hadoop-yarn/staging
drwxrwx---   - hadoop supergroup          0 2017-06-07 15:13 /tmp/hadoop-yarn/staging/history
drwxrwx---   - hadoop supergroup          0 2017-06-07 15:13 /tmp/hadoop-yarn/staging/history/done
drwxrwxrwt   - hadoop supergroup          0 2017-06-07 15:13 /tmp/hadoop-yarn/staging/history/done_intermediate

[hadoop@client ~]$       hdfs dfs -chmod 777 /tmp
[hadoop@client ~]$       hdfs dfs -mkdir /user
[hadoop@client ~]$       hdfs dfs -mkdir /user/hadoop
[hadoop@client ~]$       echo 'test' > test.txt
[hadoop@client ~]$       hdfs dfs -put test.txt /user/hadoop/hadooptest.txt
[hadoop@client ~]$       hdfs dfs -ls -R /user
drwxr-xr-x   - hadoop supergroup          0 2017-06-07 15:20 /user/hadoop
-rw-r--r--   1 hadoop supergroup          5 2017-06-07 15:20 /user/hadoop/hadooptest.txt
[hadoop@client ~]$       hdfs dfs -ls
Found 1 items
-rw-r--r--   1 hadoop supergroup          5 2017-06-07 15:20 hadooptest.txt
[hadoop@client ~]$
~~~
(12) Pig로 테스트하기


[NOTE] Pig는 클라이언트 머신에서 실행함
~~~
[1]$ hostname
~~~
- Pig 설치 확인
~~~
[2]$ ls
~~~
- 테스트용 파일 만들어 HDFS에 올리기
~~~
[3]$ echo 'aaa,100' > pig.txt
     echo 'bbb,200' >> pig.txt
     echo 'ccc,300' >> pig.txt
     echo 'aaa,400' >> pig.txt

[4]$ hdfs dfs -put  pig.txt  /user/hadoop/pig.txt

[5]$ hdfs dfs -ls  .

[6]$ hdfs dfs -cat   pig.txt
~~~
- Pig의 CLI인 Grunt 쉘에 접속하기
~~~
[7]$ pig
~~~
~~~
[hadoop@client ~]$ hostname
client.hadoop.kr
[hadoop@client ~]$ ls
apache-hive-1.2.1-bin  mysql-connector-java-5.1.23-bin.jar
bashrc2.txt            org_bashrc
etc_hadoop_2017        pig
hadoop                 pig-0.16.0
hadoop-2.6.4           spark
hdconf_2017.tgz        spark-1.6.2-bin-hadoop2.6
hive                   test.txt
hive-1.2.1-site.xml
[hadoop@client ~]$ echo 'aaa,100' > pig.txt
[hadoop@client ~]$      echo 'bbb,200' >> pig.txt
[hadoop@client ~]$      echo 'ccc,300' >> pig.txt
[hadoop@client ~]$      echo 'aaa,400' >> pig.txt
[hadoop@client ~]$ hdfs dfs -put  pig.txt  /user/hadoop/pig.txt
[hadoop@client ~]$ hdfs dfs -ls  .
Found 2 items
-rw-r--r--   1 hadoop supergroup          5 2017-06-07 15:20 hadooptest.txt
-rw-r--r--   1 hadoop supergroup         32 2017-06-07 15:26 pig.txt
[hadoop@client ~]$ hdfs dfs -cat   pig.txt
aaa,100
bbb,200
ccc,300
aaa,400
[hadoop@client ~]$ pig
....
....
grunt>
~~~

**Grunt 쉘에서 테스트하기**
~~~
grunt> ls
grunt> cat  pig.txt
grunt> k2 = load 'pig.txt' using PigStorage(',')
                  as ( str:chararray, price:int );
grunt> k4 = GROUP k2 BY $0;
grunt> k8 = foreach k4 generate group, SUM(k2.price);
grunt> dump  k8;
grunt> store  k8  into 'mr_pig';
grunt> cat  mr_pig
grunt> illustrate  k8;
grunt> quit
~~~


~~~
grunt> ls
hdfs://namenode.hadoop.kr:8020/user/hadoop/hadooptest.txt<r 1>  5
hdfs://namenode.hadoop.kr:8020/user/hadoop/pig.txt<r 1> 32

grunt> cat  pig.txt
aaa,100
bbb,200
ccc,300
aaa,400

grunt> k2 = load 'pig.txt' using PigStorage(',')
>>                   as ( str:chararray, price:int );
2017-05-31 13:28:00,902 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS

grunt> k4 = GROUP k2 BY $0;

grunt> k8 = foreach k4 generate group, SUM(k2.price);

grunt> dump  k8;
...
[ MapReduce가 실행됨 => 결과값을 바로 확인 ]
...
(aaa,500)
(bbb,200)
(ccc,300)

grunt> store  k8  into 'mr_pig';
...
[ MapReduce가 실행됨 => 'mr_pig'에 결과값 저장 ]
...
2017-05-31 13:35:15,901 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!

grunt> cat  mr_pig
aaa     500
bbb     200
ccc     300

grunt> illustrate  k8;
--------------------------------------------
| k2     | str:chararray    | price:int    |
--------------------------------------------
|        | aaa              | 400          |
|        | aaa              | 100          |
--------------------------------------------
---------------------------------------------------------------------------------------
| k4     | group:chararray    | k2:bag{:tuple(str:chararray,price:int)}               |
---------------------------------------------------------------------------------------
|        | aaa                | {}                                                    |
|        | aaa                | {}                                                    |
---------------------------------------------------------------------------------------
-------------------------------------------
| k8     | group:chararray    | :long     |
-------------------------------------------
|        | aaa                | 500       |
-------------------------------------------

grunt> quit;
2017-05-31 13:38:13,071 [main] INFO  org.apache.pig.Main - Pig script completed in 12 minutes, 28 seconds and 538 milliseconds (748538 ms)
[hadoop@client ~]$
~~~


* pig의 grunt>에 접속해 있을 때
  1번 머신 = 클라이언트 노드의 jps 결과
~~~
[hadoop@client ~]$ jps
1813 Jps
1705 RunJar => pig의 grunt> 쉘
[hadoop@client ~]$
~~~

* pig의 grunt>에서 맵리듀스를 실행할 때
  2번 머신 = 네임노드의 jps 결과
~~~
[hadoop@client ~]$ ssh namenode
Last login: Wed Jun  7 15:47:50 2017 from client.hadoop.kr

[hadoop@namenode ~]$ jps
2017 NameNode
3747 MRAppMaster       => 맵리듀스 어플리케이션 마스터
2645 JobHistoryServer
3917 Jps
3878 YarnChild         => 맵리듀스 태스크 콘테이너
2427 NodeManager
2139 DataNode
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@client ~]$
~~~
### (13) 하둡 서비스 중단하기

**하둡 서비스 중단하기**

=> 서비스 시작 순서
    HDFS -> YARN -> MR-History Server

=> 서비스 중단하기 순서
    YARN -> MR-History Server -> HDFS

[NOTE] HDFS 서비스를 처음 시작하고 마지막에 종료


-> 1번 머신에서 3번 머신으로 SSH 접속

[1]$ ssh secondnode

- YARN 서비스 중단하기

[2]$ hadoop/sbin/stop-yarn.sh

-> 3번 머신에서 자바 데몬 확인

[3]$ jps

-> 3번 머신에서 2번 머신으로 SSH 접속

[4]$ ssh namenode

- MR-History Server 중단하기

[5]$ hadoop/sbin/mr-jobhistory-daemon.sh stop historyserver

- HDFS 서비스 중단하기

[6]$ hadoop/sbin/stop-dfs.sh

-> 2번 머신에서 HDFS 자바 데몬 확인
[7]$ jps

<- 로그아웃하여 다시 3번 머신으로
[8]$ exit

<- 로그아웃하여 다시 1번 머신으로
[9]$ exit
~~~
[hadoop@client ~]$ ssh secondnode
Last login: Wed Jun  7 15:47:23 2017 from namenode.hadoop.kr
[hadoop@secondnode ~]$ hadoop/sbin/stop-yarn.sh
stopping yarn daemons
stopping resourcemanager
datanode2.hadoop.kr: stopping nodemanager
datanode1.hadoop.kr: stopping nodemanager
datanode2.hadoop.kr: nodemanager did not stop gracefully after 5 seconds: killing with kill -9
datanode1.hadoop.kr: nodemanager did not stop gracefully after 5 seconds: killing with kill -9
no proxyserver to stop
[hadoop@secondnode ~]$ jps
1147 DataNode
1235 SecondaryNameNode
2686 Jps
[hadoop@secondnode ~]$ ssh namenode
Last login: Wed Jun  7 15:50:09 2017 from client.hadoop.kr
[hadoop@namenode ~]$ hadoop/sbin/mr-jobhistory-daemon.sh stop historyserver
stopping historyserver
[hadoop@namenode ~]$ hadoop/sbin/stop-dfs.sh
Stopping namenodes on [namenode.hadoop.kr]
namenode.hadoop.kr: stopping namenode
datanode2.hadoop.kr: stopping datanode
datanode1.hadoop.kr: stopping datanode
Stopping secondary namenodes [secondnode.hadoop.kr]
secondnode.hadoop.kr: stopping secondarynamenode
[hadoop@namenode ~]$ jps
4425 Jps
[hadoop@namenode ~]$ exit
logout
Connection to namenode closed.
[hadoop@secondnode ~]$ exit
logout
Connection to secondnode closed.
[hadoop@client ~]$

~~~

# 하둡 기본 명령어
# 샘플 데이터
